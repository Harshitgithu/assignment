{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e995d9f7-3137-4953-994e-6d8170002fec",
   "metadata": {},
   "source": [
    "## Question-1:What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ca368c3-ef68-4384-b63a-ebca2f7b4d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is a regularization technique used in feature selection using a Shrinkage method also referred to as the penalized regression method. Lasso is short for Least Absolute Shrinkage and Selection Operator, which is used both for regularization and model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bb6a78-ad74-448d-bec6-c8ce0620d602",
   "metadata": {},
   "source": [
    "## Question-2:What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5617108-a241-4f40-b6d6-a67c26afb720",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The main advantage of a LASSO regression model is that it has the ability to set the coefficients for features it does not consider interesting to zero. This means that the model does some automatic feature selection to decide which features should and should not be included on its own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b23c0be-26a8-49bf-a5dd-6b8e3c58413a",
   "metadata": {},
   "source": [
    "## Question-3 How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02e40ae0-0903-4ba3-9859-43b1edf7cbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lasso regression uses shrinkage, where the data values are shrunk towards a central point such as the mean value. The Lasso penalty shrinks or reduces the coefficient value towards zero. The less contributing variable is therefore allowed to have a zero or near-zero coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e406875-258b-4ee8-bda4-23447f453c26",
   "metadata": {},
   "source": [
    "## Question-4 :What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8db8d585-1fe8-466e-a995-90998a284dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This property of LASSO makes it useful for feature selection, as the variables with zero coefficients are effectively removed from the model. Tuning parameter λ : The choice of the regularization parameter λ is crucial in LASSO regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e59467-9d1f-423a-99b2-991456a5db2d",
   "metadata": {},
   "source": [
    "## Question-5 :Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a26ef8ce-91a8-4389-afc9-6a559b980972",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Regularization with a lasso penalty is an advantageous in that it estimates some coefficients in linear regression models to be exactly zero. We propose imposing a weighted lasso penalty on a nonlinear regression model and thereby selecting the number of basis functions effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7a1e19-12c5-4042-88ff-73ddcf8bcb1e",
   "metadata": {},
   "source": [
    "## Question-6 :What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79343ad1-05e4-4ba1-8fbd-fa68ede0c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The difference between ridge and lasso regression is that it tends to make coefficients to absolute zero as compared to Ridge which never sets the value of coefficient to absolute zero. Limitation of Lasso Regression: Lasso sometimes struggles with some types of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14475403-8b20-4b5c-a0bb-149c4a8e14c2",
   "metadata": {},
   "source": [
    "## Question-7 :Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f960612-8e66-43ed-8124-188fe3a517c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Luckily, because of LASSO's built-in variable selection, it can handle some multicollinearity without sacrificing interpretability. If the collinearity is too high, however, LASSO's variable selection performance will start to suffer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1f1123-6536-45b2-9cb5-09b6a2fde6ea",
   "metadata": {},
   "source": [
    "## Question-8: How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "781139c4-d4c1-4b6b-b8a1-faefeced1ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The best cross-validation score is obtained for the 0.4 value of lambda. This is your optimal value of lambda. This is how we choose the estimated best model with optimal hyper-parameter values. Use this same process with different types of algorithms like Ridge, LASSO, Elastic-Net, Random Forests, and Boosted trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10e60e8-f2d6-46fe-843a-f4887850fb4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

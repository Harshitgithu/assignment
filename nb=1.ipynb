{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d0451e2-802f-4187-81f1-9ac08864ea5d",
   "metadata": {},
   "source": [
    "## Question-1 :What is Bayes' theorem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c5a1c7-84ad-4f63-aec5-8406c2f2f16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayes' Theorem is a fundamental concept in probability theory that describes the probability of an event based on prior knowledge of conditions that might be related to the event. It's named after Thomas Bayes, an 18th-century statistician and theologian, although the theorem was published posthumously.\n",
    "\n",
    "The theorem is expressed mathematically as\n",
    "P(A∣B)= \n",
    "P(B)\n",
    "P(B∣A)⋅P(A)\n",
    "​\n",
    " \n",
    "\n",
    "where:\n",
    "P(A∣B) is the probability of event A occurring given that event B has occurred (called the posterior probability).\n",
    "P(B∣A) is the probability of event B occurring given that event A has occurred (called the likelihood).\n",
    "P(A) is the probability of event A occurring (called the prior probability).\n",
    "P(B) is the probability of event B occurring.\n",
    "Bayes' Theorem is commonly used in statistics, machine learning, and various fields where probabilistic reasoning is applied. It is particularly useful in situations where we want to update our beliefs about an event based on new evidence.\n",
    "\n",
    "Example:\n",
    "Let's say we have a medical test for a rare disease, and the test is not perfect; it can give false positives and false negatives. We want to know the probability of having the disease given that the test result is positive.\n",
    "P(A): Probability of having the disease (prior probability)\n",
    "P(B∣A): Probability of testing positive given that the person has the disease (likelihoo\n",
    "P(B): Probability of testing positive (normalizing constant).\n",
    "P(A∣B): Probability of having the disease given a positive test result (posterior probability).\n",
    "Bayes' Theorem allows us to update our belief about the probability of having the disease based on the test result and the prior probability of having the disease. It provides a systematic way to incorporate new evidence into our existing beliefs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdef868-8446-49b9-8ed9-826df9d9d82c",
   "metadata": {},
   "source": [
    "## Question-2 :What is the formula for Bayes' theorem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce57c4d2-fd8d-4154-9ad5-cd95cc15c20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayes' Theorem is a mathematical formula that describes the probability of an event based on prior knowledge of conditions that might be related to the event. The formula is expressed as:\n",
    "P(A∣B)= \n",
    "P(B)\n",
    "P(B∣A)⋅P(A)\n",
    "​\n",
    " \n",
    "\n",
    "where:\n",
    "\n",
    "P(A∣B) is the probability of event A occurring given that event B has occurred (called the posterior probability\n",
    "P(B∣A) is the probability of event B occurring given that event A has occurred (called the likelihood\n",
    "P(A) is the probability of event A occurring (called the prior probability\n",
    "P(B) is the probability of event B occurring.\n",
    "The terms in the formula can be interpreted as follows:\n",
    "\n",
    "Posterior Probability \n",
    "P(A∣B)): The updated probability of event A occurring after taking into account the occurrence of event B.\n",
    "Likelihood \n",
    "P(B∣A)): The probability of observing event B given that event A has occurred.\n",
    "Prior Probability \n",
    "P(A)): The initial probability of event A occurring before observing event B.\n",
    "Normalizing Constant \n",
    "P(B)): The probability of observing event B, which serves as a normalizing factor to ensure that the posterior probability is a valid probability distribution.\n",
    "Bayes' Theorem is widely used in statistics, machine learning, and various fields for making predictions, updating beliefs, and reasoning under uncertainty. It provides a formal way to incorporate new evidence into existing beliefs, making it a fundamental tool in probabilistic reasoning.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a8a7be-7011-41ec-a6c2-e12b4d8a2b4b",
   "metadata": {},
   "source": [
    "## Question-3 :How is Bayes' theorem used in practice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104dd6fa-25b6-4f18-9d33-ddc85e0ec0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayes' Theorem is used in practice in various fields for making predictions, updating beliefs, and reasoning under uncertainty. Its application involves updating probabilities based on new evidence, allowing for more informed decision-making. Here are some practical applications of Bayes' Theorem:\n",
    "\n",
    "Medical Diagnosis:\n",
    "\n",
    "Bayes' Theorem is widely used in medical diagnosis, especially when interpreting the results of diagnostic tests. It helps calculate the probability of a disease given the test results and prior knowledge of the patient's health.\n",
    "Spam Filtering:\n",
    "\n",
    "In email spam filtering, Bayes' Theorem is applied to classify emails as spam or not spam based on the occurrence of specific words. This is the basis for techniques like Naive Bayes classifiers in spam detection.\n",
    "Weather Prediction:\n",
    "\n",
    "Bayes' Theorem can be used in weather prediction to update the probability of certain weather conditions based on new observations, such as current temperature, humidity, and wind speed.\n",
    "Finance and Investment:\n",
    "\n",
    "In finance, Bayes' Theorem is applied to update probabilities of financial events based on new market information, helping investors make more informed decisions.\n",
    "Machine Learning and Classification:\n",
    "\n",
    "Bayesian methods are employed in machine learning for various tasks, including classification. For example, Bayesian classifiers, such as Naive Bayes, use Bayes' Theorem to predict the class of an observation based on the occurrence of features.\n",
    "Speech and Language Processing:\n",
    "\n",
    "In natural language processing, Bayes' Theorem is used in tasks like language modeling and part-of-speech tagging, where the probability of a word or a sequence of words is updated based on context.\n",
    "Fault Diagnosis in Engineering:\n",
    "\n",
    "In engineering, Bayes' Theorem is applied to diagnose faults in systems. By updating probabilities based on observed symptoms, it helps identify the most likely causes of malfunctions.\n",
    "A/B Testing in Marketing:\n",
    "\n",
    "In marketing, A/B testing involves comparing two versions (A and B) of a webpage, email, or other marketing elements. Bayes' Theorem is applied to update probabilities of conversion rates based on observed user behavior, helping decide which version is more effective.\n",
    "Epidemiology:\n",
    "\n",
    "Bayes' Theorem is used in epidemiology to estimate the probability of a person having a certain disease based on factors such as exposure to risk factors.\n",
    "In each of these applications, Bayes' Theorem provides a formal and systematic way to update probabilities based on new evidence, allowing for better-informed decision-making in situations where uncertainty and incomplete information are common."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cc9121-bff4-4722-9eb9-09af85dd4e4f",
   "metadata": {},
   "source": [
    "## Question-4 :What is the relationship between Bayes' theorem and conditional probability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fab00b3-7e44-4b31-bd45-6a85b6135548",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayes' Theorem and conditional probability are closely related concepts in probability theory. Bayes' Theorem is essentially a formula that relates conditional probabilities in a specific way. Let's first define conditional probability, and then discuss how Bayes' Theorem is derived from it.\n",
    "\n",
    "Conditional Probability:\n",
    "Conditional probability is the probability of an event occurring given that another event has already occurred. It is denoted as\n",
    "P(A∣B), read as \"the probability of event A given event B.\" The formula for conditional probability is:\n",
    "P(A∣B)= \n",
    "P(B)\n",
    "P(A∩B)\n",
    "​\n",
    " \n",
    "\n",
    "where:\n",
    "P(A∣B) is the conditional probability of A given B\n",
    "P(A∩B) is the probability of both A and B occurring (the intersection of A and B)\n",
    "\n",
    "P(B) is the probability of event B occurring.\n",
    "Bayes' Theorem:\n",
    "Bayes' Theorem is derived from the definition of conditional probability. It provides a way to reverse the conditional probability and express \n",
    "P(A∣B) in terms of \n",
    "P(B∣A). The formula for Bayes' Theorem is:\n",
    "\n",
    "\n",
    "P(A∣B)= \n",
    "P(B)\n",
    "P(B∣A)⋅P(A)\n",
    "​\n",
    " \n",
    "\n",
    "where:\n",
    "P(A∣B) is the posterior probability of A given B.\n",
    "P(B∣A) is the likelihood of B given A.\n",
    "P(A) is the prior probability of A.\n",
    "P(B) is the probability of B.\n",
    "Relationship:\n",
    "Bayes' Theorem connects the conditional probability \n",
    "P(A∣B) with the likelihood \n",
    "P(B∣A), the prior probability \n",
    "P(A), and the probability \n",
    "P(B). It allows us to update our beliefs about the occurrence of an event based on new evidence.\n",
    "\n",
    "In summary, the relationship between Bayes' Theorem and conditional probability lies in Bayes' Theorem being a formula that expresses the posterior probability of an event given new evidence in terms of the likelihood, prior probability, and the probability of the new evidence. It provides a systematic way to update probabilities as new information becomes available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069a7ae2-7da8-42f5-82f8-e7752a34c057",
   "metadata": {},
   "source": [
    "## Question-5 :How do you choose which type of Naive Bayes classifier to use for any given problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4720e723-5748-4395-9026-77526d06efd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "The choice of which type of Naive Bayes classifier to use for a given problem depends on the nature of the data and the assumptions that can be reasonably made about the independence of features. There are three common types of Naive Bayes classifiers: Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes. Here are some guidelines for choosing the appropriate type:\n",
    "\n",
    "Gaussian Naive Bayes:\n",
    "\n",
    "Nature of Features: Suitable when the features follow a continuous distribution (e.g., real numbers).\n",
    "Example Applications:\n",
    "Natural language processing tasks when dealing with word frequencies.\n",
    "Classification problems where features have a Gaussian (normal) distribution.\n",
    "Multinomial Naive Bayes:\n",
    "\n",
    "Nature of Features: Suitable for discrete data, especially when representing the frequency of multiple categories.\n",
    "Example Applications:\n",
    "Text classification, such as spam detection or topic categorization, where features represent word counts or frequencies.\n",
    "Document classification tasks.\n",
    "Bernoulli Naive Bayes:\n",
    "\n",
    "Nature of Features: Appropriate for binary features (0 or 1), representing the presence or absence of a particular feature.\n",
    "Example Applications:\n",
    "Document classification based on the presence or absence of specific words.\n",
    "Sentiment analysis where features indicate the presence or absence of certain words or phrases.\n",
    "Guidelines for Selection:\n",
    "Nature of Data:\n",
    "\n",
    "Consider the nature of your features. If they are continuous, Gaussian Naive Bayes might be suitable. For discrete features, choose between Multinomial and Bernoulli Naive Bayes based on whether they represent counts or binary occurrences.\n",
    "Assumptions about Independence:\n",
    "\n",
    "Naive Bayes classifiers assume that features are conditionally independent given the class label. Assess whether this assumption is reasonable for your data. In practice, Naive Bayes can perform well even if the independence assumption is not strictly met.\n",
    "Size of Dataset:\n",
    "\n",
    "For small datasets or datasets with limited labeled examples, Naive Bayes classifiers can be particularly useful due to their simplicity and low computational requirements.\n",
    "Performance in Practice:\n",
    "\n",
    "Experiment with different types of Naive Bayes classifiers and evaluate their performance on your specific problem using cross-validation or other model evaluation techniques.\n",
    "Consider Preprocessing:\n",
    "\n",
    "Preprocess your data accordingly. For example, in text classification tasks, you might use techniques like TF-IDF (Term Frequency-Inverse Document Frequency) before applying a Multinomial Naive Bayes classifier.\n",
    "In many cases, it's advisable to try multiple Naive Bayes classifiers and compare their performance on a validation set or through cross-validation. The choice might also depend on the specific requirements of the problem at hand and the characteristics of the available data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc949cd-c87a-49ef-9b24-2f0a039cd9f4",
   "metadata": {},
   "source": [
    "## Question-6 :Assignment:\n",
    "## You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive\n",
    "## Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of\n",
    "## each feature value for each class:\n",
    "## Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
    "## A 3 3 4 4 3 3 3\n",
    "## B 2 2 1 2 2 2 3\n",
    "## Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance\n",
    "## to belong to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d87b786-06be-45cf-ab60-850ba4353d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "To classify a new instance using Naive Bayes, we need to calculate the likelihoods and prior probabilities for each class based on the given dataset. Assuming equal prior probabilities for each class, the class with the highest posterior probability will be the predicted class.\n",
    "\n",
    "Let:\n",
    "\n",
    "�\n",
    "1\n",
    "=\n",
    "3\n",
    "X \n",
    "1\n",
    "​\n",
    " =3 and \n",
    "�\n",
    "2\n",
    "=\n",
    "4\n",
    "X \n",
    "2\n",
    "​\n",
    " =4 be the features of the new instance.\n",
    "We need to calculate \n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    "1\n",
    "=\n",
    "3\n",
    ",\n",
    "�\n",
    "2\n",
    "=\n",
    "4\n",
    ")\n",
    "P(A∣X \n",
    "1\n",
    "​\n",
    " =3,X \n",
    "2\n",
    "​\n",
    " =4) and \n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    "1\n",
    "=\n",
    "3\n",
    ",\n",
    "�\n",
    "2\n",
    "=\n",
    "4\n",
    ")\n",
    "P(B∣X \n",
    "1\n",
    "​\n",
    " =3,X \n",
    "2\n",
    "​\n",
    " =4) using Bayes' Theorem:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    "1\n",
    "=\n",
    "3\n",
    ",\n",
    "�\n",
    "2\n",
    "=\n",
    "4\n",
    ")\n",
    "=\n",
    "�\n",
    "(\n",
    "�\n",
    "1\n",
    "=\n",
    "3\n",
    ",\n",
    "�\n",
    "2\n",
    "=\n",
    "4\n",
    "∣\n",
    "�\n",
    ")\n",
    "⋅\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "�\n",
    "(\n",
    "�\n",
    "1\n",
    "=\n",
    "3\n",
    ",\n",
    "�\n",
    "2\n",
    "=\n",
    "4\n",
    ")\n",
    "P(A∣X \n",
    "1\n",
    "​\n",
    " =3,X \n",
    "2\n",
    "​\n",
    " =4)= \n",
    "P(X \n",
    "1\n",
    "​\n",
    " =3,X \n",
    "2\n",
    "​\n",
    " =4)\n",
    "P(X \n",
    "1\n",
    "​\n",
    " =3,X \n",
    "2\n",
    "​\n",
    " =4∣A)⋅P(A)\n",
    "​\n",
    " \n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    "∣\n",
    "�\n",
    "1\n",
    "=\n",
    "3\n",
    ",\n",
    "�\n",
    "2\n",
    "=\n",
    "4\n",
    ")\n",
    "=\n",
    "�\n",
    "(\n",
    "�\n",
    "1\n",
    "=\n",
    "3\n",
    ",\n",
    "�\n",
    "2\n",
    "=\n",
    "4\n",
    "∣\n",
    "�\n",
    ")\n",
    "⋅\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "�\n",
    "(\n",
    "�\n",
    "1\n",
    "=\n",
    "3\n",
    ",\n",
    "�\n",
    "2\n",
    "=\n",
    "4\n",
    ")\n",
    "P(B∣X \n",
    "1\n",
    "​\n",
    " =3,X \n",
    "2\n",
    "​\n",
    " =4)= \n",
    "P(X \n",
    "1\n",
    "​\n",
    " =3,X \n",
    "2\n",
    "​\n",
    " =4)\n",
    "P(X \n",
    "1\n",
    "​\n",
    " =3,X \n",
    "2\n",
    "​\n",
    " =4∣B)⋅P(B)\n",
    "​\n",
    " \n",
    "\n",
    "Given the table of frequencies, we can estimate the probabilities as follows:\n",
    "\n",
    "Prior Probabilities:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "0.5\n",
    "P(A)=P(B)=0.5 (equal prior probabilities).\n",
    "Likelihoods:\n",
    "\n",
    "P(X \n",
    "1\n",
    "​\n",
    " =3,X \n",
    "2\n",
    "​\n",
    " =4∣A)=P(X \n",
    "1\n",
    "​\n",
    " =3∣A)⋅P(X \n",
    "2\n",
    "​\n",
    " =4∣A) (assuming independence)\n",
    "P(X \n",
    "1\n",
    "​\n",
    " =3,X \n",
    "2\n",
    "​\n",
    " =4∣B)=P(X \n",
    "1\n",
    "​\n",
    " =3∣B)⋅P(X \n",
    "2\n",
    "​\n",
    " =4∣B) (assuming independence)\n",
    "\n",
    "P(X \n",
    "1\n",
    "​\n",
    " =3∣A)= \n",
    "10\n",
    "4\n",
    "​\n",
    " \n",
    "\n",
    "\n",
    "10\n",
    "P(X \n",
    "2\n",
    "​\n",
    " =4∣A)= \n",
    "10\n",
    "3\n",
    "\n",
    "P(X \n",
    "1\n",
    "​\n",
    " =3∣B)= \n",
    "5\n",
    "1\n",
    "​\n",
    " \n",
    "P(X \n",
    "2\n",
    "​\n",
    " =4∣B)= \n",
    "5\n",
    "3\n",
    "​\n",
    " \n",
    "\n",
    "Posterior Probabilities:\n",
    "\n",
    "Use Bayes' Theorem to calculate the posterior probabilities.\n",
    "P(A∣X \n",
    "1\n",
    "​\n",
    " =3,X \n",
    "2\n",
    "​\n",
    " =4)∝P(X \n",
    "1\n",
    "​\n",
    " =3,X \n",
    "2\n",
    "​\n",
    " =4∣A)⋅P(A)\n",
    "\n",
    "P(B∣X \n",
    "1\n",
    "​\n",
    " =3,X \n",
    "2\n",
    "​\n",
    " =4)∝P(X \n",
    "1\n",
    "​\n",
    " =3,X \n",
    "2\n",
    "​\n",
    " =4∣B)⋅P(B)\n",
    "\n",
    "Normalize the probabilities to make them sum to 1.\n",
    "\n",
    "After calculating the posterior probabilities, the class with the higher probability will be the predicted class for the new instance.\n",
    "\n",
    "Please note that the calculations involve proportionalities due to normalization, and you would need to compute the exact values to compare the probabilities accurately.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

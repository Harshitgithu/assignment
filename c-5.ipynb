{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74ac8073-c867-41da-a54c-830858e1b659",
   "metadata": {},
   "source": [
    "## Question-1 : What is a contingency matrix, and how is it used to evaluate the performance of a classification model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2b3737-3e28-43e0-8227-1c35d32fa269",
   "metadata": {},
   "outputs": [],
   "source": [
    "A contingency matrix, also known as a confusion matrix, is a table used in the evaluation of the performance of a classification model. It provides a summary of the predicted and actual classifications for a set of instances. The matrix is particularly useful when dealing with binary or multi-class classification problems.\n",
    "\n",
    "Here is a typical layout of a binary classification confusion matrix:\n",
    "\n",
    "mathematica\n",
    "Copy code\n",
    "                | Predicted Negative | Predicted Positive |\n",
    "----------------|---------------------|--------------------|\n",
    "Actual Negative | True Negative (TN)  | False Positive (FP)|\n",
    "Actual Positive | False Negative (FN) | True Positive (TP) |\n",
    "In this matrix:\n",
    "\n",
    "True Positive (TP): Instances that are correctly predicted as positive.\n",
    "True Negative (TN): Instances that are correctly predicted as negative.\n",
    "False Positive (FP): Instances that are incorrectly predicted as positive (Type I error).\n",
    "False Negative (FN): Instances that are incorrectly predicted as negative (Type II error).\n",
    "Usage of Contingency Matrix:\n",
    "\n",
    "Accuracy:\n",
    "\n",
    "Accuracy is a general measure of how well the model is performing. It is calculated as \n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "TP+TN+FP+FN\n",
    "TP+TN\n",
    "​\n",
    " , representing the proportion of correctly classified instances.\n",
    "Precision (Positive Predictive Value):\n",
    "\n",
    "Precision measures the accuracy of positive predictions and is calculated as \n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "TP+FP\n",
    "TP\n",
    "​\n",
    " . It gives an indication of how many of the predicted positive instances are actually positive.\n",
    "Recall (Sensitivity, True Positive Rate):\n",
    "\n",
    "Recall measures the ability of the model to capture all positive instances and is calculated as \n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "TP+FN\n",
    "TP\n",
    "​\n",
    " . It gives an indication of how many of the actual positive instances were correctly predicted.\n",
    "Specificity (True Negative Rate):\n",
    "\n",
    "Specificity measures the ability of the model to avoid false positives and is calculated as \n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "TN+FP\n",
    "TN\n",
    "​\n",
    " . It gives an indication of how well the model distinguishes negative instances.\n",
    "F1 Score:\n",
    "\n",
    "The F1 score is the harmonic mean of precision and recall, providing a balance between the two. It is calculated as \n",
    "2\n",
    "×\n",
    "Precision\n",
    "×\n",
    "Recall\n",
    "Precision\n",
    "+\n",
    "Recall\n",
    "2× \n",
    "Precision+Recall\n",
    "Precision×Recall\n",
    "​\n",
    " .\n",
    "Matthews Correlation Coefficient (MCC):\n",
    "\n",
    "The MCC takes into account all four values in the contingency matrix and is particularly useful for imbalanced datasets. It is calculated as \n",
    "�\n",
    "�\n",
    "×\n",
    "�\n",
    "�\n",
    "−\n",
    "�\n",
    "�\n",
    "×\n",
    "�\n",
    "�\n",
    "(\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    ")\n",
    "(\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    ")\n",
    "(\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    ")\n",
    "(\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    ")\n",
    "(TP+FP)(TP+FN)(TN+FP)(TN+FN)\n",
    "​\n",
    " \n",
    "TP×TN−FP×FN\n",
    "​\n",
    " .\n",
    "These metrics are computed using values from the contingency matrix and provide different aspects of the model's performance. The choice of which metric(s) to emphasize depends on the specific goals and requirements of the classification task. Additionally, the contingency matrix is not limited to binary classification and can be extended to multi-class problems by considering all possible classes.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcae350-7b98-451a-b3fa-b5defb7f6948",
   "metadata": {},
   "source": [
    "## Question-2 :How is a pair confusion matrix different from a regular confusion matrix, and why might it be useful in certain situations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda2e237-6955-49a1-be50-0bd1e0fa8efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "A pair confusion matrix is a variation of the traditional confusion matrix that is specifically designed for assessing the performance of binary or two-class classification models in situations where the classes are inherently paired or matched. It is particularly useful when the goal is to focus on the correct pairing of instances across classes, emphasizing the relationships between paired classes rather than individual class performance.\n",
    "\n",
    "In a pair confusion matrix, the layout is adjusted to accommodate pairs of classes, and the main components include:\n",
    "\n",
    "mathematica\n",
    "Copy code\n",
    "                      | Predicted Pair Negative | Predicted Pair Positive |\n",
    "----------------------|-------------------------|-------------------------|\n",
    "Actual Pair Negative  | True Pair Negative (TPN) | False Pair Positive (FPP)|\n",
    "Actual Pair Positive  | False Pair Negative (FPN)| True Pair Positive (TPP) |\n",
    "Here, the terms have the following meanings:\n",
    "\n",
    "True Pair Positive (TPP): Instances from a paired class that are correctly predicted as positive.\n",
    "True Pair Negative (TPN): Instances from a paired class that are correctly predicted as negative.\n",
    "False Pair Positive (FPP): Instances from a paired class that are incorrectly predicted as positive.\n",
    "False Pair Negative (FPN): Instances from a paired class that are incorrectly predicted as negative.\n",
    "Usefulness in Certain Situations:\n",
    "\n",
    "Paired Class Relationships:\n",
    "\n",
    "Pair confusion matrices are particularly useful when dealing with classification problems where instances naturally come in pairs or are related in some way. For example, in medical diagnostics, one may be interested in correctly identifying instances of a disease (positive class) and instances of non-disease (negative class) for each patient.\n",
    "Emphasis on Pair Correctness:\n",
    "\n",
    "The pair confusion matrix places a specific emphasis on correctly pairing instances from the same class, allowing for a more targeted assessment of the model's performance in recognizing relationships between paired classes.\n",
    "Reduction of Complexity:\n",
    "\n",
    "In certain situations, using a pair confusion matrix can simplify the evaluation process, especially when there are multiple paired classes, and the focus is on the correctness of pairing rather than individual class performance.\n",
    "Balance of Sensitivity and Specificity:\n",
    "\n",
    "By considering pairs, the pair confusion matrix inherently balances sensitivity and specificity for each paired class. It allows one to evaluate how well the model distinguishes between the paired classes while accounting for potential imbalances in class sizes.\n",
    "Application to Specific Domains:\n",
    "\n",
    "Pair confusion matrices are often used in domains where the relationships between two classes are critical, and misclassifying one class as the other has specific consequences. This can include applications in finance, security, and various scientific fields.\n",
    "Example:\n",
    "Consider a medical diagnosis scenario with classes \"Healthy\" and \"Diseased.\" A pair confusion matrix might be used to assess the model's performance in correctly identifying instances as either \"Healthy\" or \"Diseased\" for each patient, emphasizing the importance of correctly pairing instances for a meaningful diagnosis.\n",
    "\n",
    "While pair confusion matrices are valuable in certain situations, they are not universally applicable. In many classification tasks, the traditional confusion matrix or other evaluation metrics may be more appropriate, depending on the goals and characteristics of the problem.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0064f76e-e620-473d-a1cc-38080a0d0246",
   "metadata": {},
   "source": [
    "## Question-3 :What is an extrinsic measure in the context of natural language processing, and how is it typically used to evaluate the performance of language models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a75ef3b-9f95-4dd8-a40b-fb88052cbe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "In the context of natural language processing (NLP), extrinsic measures refer to evaluation metrics that assess the performance of language models based on their ability to contribute to a specific downstream task or application. These metrics are task-specific and are used to measure the practical utility of a language model in real-world applications.\n",
    "\n",
    "Extrinsic evaluation involves integrating the language model into an end-to-end system or pipeline designed for a particular NLP task. The performance of the language model is then assessed based on the overall success of the entire system in achieving the task's objectives.\n",
    "\n",
    "Here's how extrinsic evaluation is typically conducted in NLP:\n",
    "\n",
    "Integration into Downstream Task:\n",
    "\n",
    "The language model, which could be a pre-trained model like a language model or an embedding model, is integrated into a system or pipeline designed for a specific downstream NLP task. Downstream tasks can include sentiment analysis, named entity recognition, machine translation, question answering, etc.\n",
    "End-to-End Evaluation:\n",
    "\n",
    "The entire system, including the language model, is evaluated in an end-to-end manner on the target task. This involves feeding input data relevant to the task into the system and assessing the quality and correctness of the output produced by the system.\n",
    "Task-Specific Metrics:\n",
    "\n",
    "Performance is measured using task-specific metrics relevant to the downstream application. For example:\n",
    "In sentiment analysis, accuracy or F1 score might be used.\n",
    "In named entity recognition, precision, recall, and F1 score might be used.\n",
    "In machine translation, BLEU score or METEOR score might be used.\n",
    "Real-World Applicability:\n",
    "\n",
    "Extrinsic measures are valuable because they provide insights into how well a language model performs in real-world applications. These metrics go beyond general language understanding or generation capabilities and focus on the model's effectiveness in solving specific problems.\n",
    "Consideration of Task Objectives:\n",
    "\n",
    "Extrinsic measures align with the objectives of the downstream task. If the ultimate goal is to improve translation quality or enhance sentiment analysis accuracy, the extrinsic evaluation captures the model's contribution to achieving these objectives.\n",
    "Extrinsic evaluation contrasts with intrinsic evaluation, which involves assessing language models based on their performance on isolated linguistic tasks or benchmarks that are not directly tied to real-world applications. Intrinsic measures might include perplexity, word similarity tasks, or syntactic parsing accuracy.\n",
    "\n",
    "While intrinsic measures provide insights into the language model's linguistic capabilities, extrinsic measures are crucial for understanding how well the model translates those capabilities into practical utility within specific applications. Both types of evaluation are often used together to gain a comprehensive understanding of a language model's overall performance and limitations.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27a3a5d-2689-4531-acde-f7425675c14b",
   "metadata": {},
   "source": [
    "## Question-4 :What is an intrinsic measure in the context of machine learning, and how does it differ from an extrinsic measure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1806c4b1-adf8-4935-965a-c14cf66ede82",
   "metadata": {},
   "outputs": [],
   "source": [
    "In the context of machine learning, intrinsic measures and extrinsic measures refer to two different approaches for evaluating the performance of models. These evaluation measures help assess the capabilities and limitations of machine learning models in various scenarios.\n",
    "\n",
    "Intrinsic Measure:\n",
    "\n",
    "An intrinsic measure evaluates a model's performance based on its performance on isolated, well-defined tasks or benchmarks that are specific to the model's core functionalities. These tasks are often chosen to focus on specific aspects of the model's capabilities, such as its ability to understand language, generate coherent text, or recognize patterns.\n",
    "\n",
    "Examples of intrinsic measures include perplexity in language modeling, accuracy in word similarity tasks, precision and recall in named entity recognition, or accuracy in part-of-speech tagging. These tasks are designed to measure specific aspects of a model's performance in controlled environments.\n",
    "\n",
    "Extrinsic Measure:\n",
    "\n",
    "An extrinsic measure evaluates a model's performance based on its contribution to the success of an end-to-end application or downstream task. In extrinsic evaluation, the model is integrated into a broader system or pipeline designed for a specific real-world application. The evaluation is based on the overall success of the system in achieving its objectives.\n",
    "\n",
    "Examples of extrinsic measures include accuracy in sentiment analysis, BLEU score in machine translation, or F1 score in named entity recognition. These metrics assess how well the model, when used as a component in a larger system, performs on tasks that have practical applications.\n",
    "\n",
    "Key Differences:\n",
    "\n",
    "Focus of Evaluation:\n",
    "\n",
    "Intrinsic measures focus on assessing a model's performance on specific, isolated tasks that are chosen to highlight particular aspects of its capabilities (e.g., language understanding, pattern recognition).\n",
    "Extrinsic measures assess a model's performance in the context of a complete application or downstream task, evaluating its overall utility and effectiveness.\n",
    "Task Isolation vs. Real-World Application:\n",
    "\n",
    "Intrinsic measures often involve tasks that are isolated from real-world applications and are specifically designed to evaluate the model's performance on a narrow aspect.\n",
    "Extrinsic measures involve using the model as part of an end-to-end system or pipeline, considering its performance in the broader context of a real-world application.\n",
    "Task-Specific vs. Task-Agnostic:\n",
    "\n",
    "Intrinsic measures are task-specific and often tailored to evaluate particular capabilities of the model relevant to a given benchmark or linguistic task.\n",
    "Extrinsic measures are task-agnostic in the sense that they focus on the model's overall contribution to achieving the objectives of a downstream task or application.\n",
    "Complexity of Evaluation:\n",
    "\n",
    "Intrinsic measures typically involve simpler and more controlled tasks, making it easier to analyze and interpret the results for specific aspects of model performance.\n",
    "Extrinsic measures are more complex as they require assessing the model's performance in the context of a complete system, involving multiple components and potential interactions.\n",
    "In practice, both intrinsic and extrinsic measures are valuable for a comprehensive evaluation of machine learning models. Intrinsic measures help researchers and practitioners understand specific strengths and weaknesses of models, while extrinsic measures provide insights into how well models perform in real-world applications. Using a combination of both types of evaluation allows for a more holistic understanding of a model's capabilities and limitations.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ba9dd6-43a9-4f77-99a8-d79f70746a6a",
   "metadata": {},
   "source": [
    "## Question-5 :What is the purpose of a confusion matrix in machine learning, and how can it be used to identify strengths and weaknesses of a model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c96c3d2-cd46-4099-b97b-6e45ab61c69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "A confusion matrix is a tool used in machine learning to evaluate the performance of a classification model. It provides a comprehensive summary of the model's predictions compared to the actual ground truth across different classes. The main purpose of a confusion matrix is to help assess the strengths and weaknesses of a model by breaking down its performance into various metrics.\n",
    "\n",
    "Here is the typical layout of a confusion matrix for a binary classification problem:\n",
    "\n",
    "mathematica\n",
    "Copy code\n",
    "                | Predicted Negative | Predicted Positive |\n",
    "----------------|---------------------|--------------------|\n",
    "Actual Negative | True Negative (TN)  | False Positive (FP)|\n",
    "Actual Positive | False Negative (FN) | True Positive (TP) |\n",
    "For a multi-class classification problem, the matrix would have a similar structure with rows and columns corresponding to each class.\n",
    "\n",
    "Key Metrics Derived from a Confusion Matrix:\n",
    "\n",
    "Accuracy:\n",
    "\n",
    "Accuracy is the overall correctness of the model and is calculated as \n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "TP+TN+FP+FN\n",
    "TP+TN\n",
    "​\n",
    " .\n",
    "Precision (Positive Predictive Value):\n",
    "\n",
    "Precision measures the accuracy of positive predictions and is calculated as \n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "TP+FP\n",
    "TP\n",
    "​\n",
    " . It represents the ability of the model to avoid false positives.\n",
    "Recall (Sensitivity or True Positive Rate):\n",
    "\n",
    "Recall measures the ability of the model to capture all positive instances and is calculated as \n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "TP+FN\n",
    "TP\n",
    "​\n",
    " . It represents the model's ability to avoid false negatives.\n",
    "Specificity (True Negative Rate):\n",
    "\n",
    "Specificity measures the ability of the model to avoid false positives in the negative class and is calculated as \n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "+\n",
    "�\n",
    "�\n",
    "TN+FP\n",
    "TN\n",
    "​\n",
    " .\n",
    "F1 Score:\n",
    "\n",
    "The F1 score is the harmonic mean of precision and recall, providing a balanced measure of a model's performance.\n",
    "Using a Confusion Matrix to Identify Strengths and Weaknesses:\n",
    "\n",
    "Diagnosing Misclassifications:\n",
    "\n",
    "The confusion matrix provides insights into specific types of misclassifications, such as false positives and false negatives. Examining these misclassifications can help understand where the model is struggling.\n",
    "Balancing Precision and Recall:\n",
    "\n",
    "Precision and recall offer a trade-off. A model with high precision tends to have lower recall and vice versa. Deciding which metric to prioritize depends on the specific goals of the task.\n",
    "Handling Class Imbalances:\n",
    "\n",
    "In imbalanced datasets, where one class has significantly more instances than others, the confusion matrix helps assess the impact on model performance. It allows for evaluating whether the model is biased toward the majority class.\n",
    "Threshold Tuning:\n",
    "\n",
    "By adjusting the classification threshold, the trade-off between precision and recall can be modified. This is particularly important when dealing with applications where false positives or false negatives have different consequences.\n",
    "Model Comparison:\n",
    "\n",
    "Comparing confusion matrices of different models or model versions can help identify improvements or regressions in performance.\n",
    "Identifying Biases:\n",
    "\n",
    "The confusion matrix can reveal biases in a model's predictions. For example, a model may perform well on certain classes but struggle with others.\n",
    "In summary, a confusion matrix is a powerful tool in machine learning for understanding the performance of a classification model. It allows for a detailed analysis of different aspects of model behavior, aiding in the identification of strengths, weaknesses, and areas for improvement.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030edd7a-c1b7-4cab-a33f-d9412b8f2df0",
   "metadata": {},
   "source": [
    "## Question-6 :What are some common intrinsic measures used to evaluate the performance of unsupervised learning algorithms, and how can they be interpreted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97688ab7-79d4-4a4d-b07a-fc27630e529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "In the context of unsupervised learning, where the algorithm aims to find patterns or structures in data without labeled target values, intrinsic measures are used to evaluate the performance of the algorithm based on its internal characteristics. These measures help assess the quality of the clusters or representations generated by the unsupervised learning algorithm. Common intrinsic measures include:\n",
    "\n",
    "Silhouette Score:\n",
    "\n",
    "The silhouette score measures how well-separated clusters are and how similar the data points within a cluster are to each other. It ranges from -1 to 1, where a higher silhouette score indicates better-defined clusters. Interpretation:\n",
    "Positive values indicate well-defined clusters with sufficient separation.\n",
    "Values near 0 suggest overlapping or poorly defined clusters.\n",
    "Negative values indicate that data points may be assigned to the wrong clusters.\n",
    "Davies-Bouldin Index:\n",
    "\n",
    "The Davies-Bouldin Index evaluates the compactness and separation between clusters. A lower Davies-Bouldin Index indicates better clustering. Interpretation:\n",
    "Lower values suggest more compact and well-separated clusters.\n",
    "Higher values indicate that clusters are either too spread out or overlapping.\n",
    "Calinski-Harabasz Index (Variance Ratio Criterion):\n",
    "\n",
    "The Calinski-Harabasz Index measures the ratio of between-cluster variance to within-cluster variance. A higher index implies better-defined clusters. Interpretation:\n",
    "Higher values indicate more distinct and well-separated clusters.\n",
    "Lower values may suggest that clusters are less defined or overlapping.\n",
    "Dunn Index:\n",
    "\n",
    "The Dunn Index assesses the compactness and separation between clusters. A higher Dunn Index indicates better clustering. Interpretation:\n",
    "Higher values suggest more compact clusters and better separation.\n",
    "Lower values may indicate overlapping or poorly defined clusters.\n",
    "Gap Statistic:\n",
    "\n",
    "The Gap Statistic compares the clustering quality of the algorithm's result to that of a random clustering. It helps determine if the algorithm's clusters are better than random chance. Interpretation:\n",
    "A higher gap statistic relative to random data suggests good clustering.\n",
    "Lower values indicate that the algorithm's clustering may not be better than random.\n",
    "Inertia (Within-Cluster Sum of Squares):\n",
    "\n",
    "Inertia measures the sum of squared distances of samples to their closest cluster center. It is often used in the context of K-means clustering. Interpretation:\n",
    "Lower inertia values indicate more compact clusters.\n",
    "Higher values suggest that data points within clusters are more spread out.\n",
    "Hopkins Statistic:\n",
    "\n",
    "The Hopkins Statistic quantifies the tendency of data points to either cluster together (indicating clustering structure) or be uniformly distributed (indicating no clear structure). Interpretation:\n",
    "A higher Hopkins Statistic suggests a higher likelihood of clustering structure.\n",
    "Interpretation Considerations:\n",
    "\n",
    "The interpretation of intrinsic measures depends on the specific goals and characteristics of the dataset. Different measures may be more suitable for different types of data or clustering algorithms.\n",
    "\n",
    "It's essential to consider the nature of the data and the assumptions of the clustering algorithm being used. Some measures may perform better on certain types of data or clustering structures.\n",
    "\n",
    "Intrinsic measures provide insights into the internal quality of clusters, but they do not necessarily reflect the external validity or utility of the clusters for a particular application.\n",
    "\n",
    "When evaluating unsupervised learning algorithms, a combination of intrinsic measures should be considered to gain a comprehensive understanding of the clustering quality. It is often helpful to experiment with different metrics and interpret the results in the context of the specific task or problem at hand.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d39759-20ef-4ff9-a291-743d346fc712",
   "metadata": {},
   "source": [
    "## Question-7 :What are some limitations of using accuracy as a sole evaluation metric for classification tasks, and how can these limitations be addressed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e46e0a-af73-4634-93cf-39658629c38f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

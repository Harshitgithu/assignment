{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d67adde-7f8c-4595-a5dc-469111a2adfb",
   "metadata": {},
   "source": [
    "## Question-1 :Explain the difference between linear regression and logistic regression models. Provide an example of a scenario where logistic regression would be more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b298e1c5-c835-4e54-8647-90ea16497577",
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear regression is a statistical method used for modeling the relationship between a dependent variable and one or more independent variables. The goal is to find the linear equation that best predicts the dependent variable based on the independent variables. The equation has the form:\n",
    "Linear regression is commonly used for predicting a continuous outcome, such as house prices, stock prices, or temperature.\n",
    "\n",
    "Logistic Regression:\n",
    "\n",
    "Logistic regression, on the other hand, is used for binary classification problems. It models the probability that a given instance belongs to a particular category. The logistic regression equation is:\n",
    "The logistic function (sigmoid) transforms the linear combination of input features into a value between 0 and 1, representing the probability of the positive class.\n",
    "\n",
    "Scenario for Logistic Regression:\n",
    "\n",
    "Logistic regression is more appropriate in scenarios where the dependent variable is binary or categorical. Here's an example:\n",
    "\n",
    "Scenario: Medical Diagnosis\n",
    "\n",
    "Suppose you are working on a medical diagnosis task, where you want to predict whether a patient has a certain medical condition (e.g., diabetes) based on several risk factors (e.g., age, BMI, blood pressure). The outcome variable is binary: either the patient has the condition (1) or does not have the condition (0).\n",
    "\n",
    "In this case, logistic regression would be suitable because it models the probability of a binary outcome. The logistic regression model would output probabilities that a given set of input features corresponds to a positive diagnosis. You could then set a threshold (e.g., 0.5) to classify instances into the positive or negative class based on these probabilities.\n",
    "\n",
    "In summary, while linear regression is used for predicting continuous outcomes, logistic regression is specifically designed for binary classification problems, making it more appropriate when dealing with scenarios where the dependent variable is categorical and has two possible outcomes.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c27d7d-d224-4e82-a795-bfbbcca49f16",
   "metadata": {},
   "source": [
    "## Question-2 :What is the cost function used in logistic regression, and how is it optimized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79825b4e-f933-490b-90cf-9dae53015600",
   "metadata": {},
   "outputs": [],
   "source": [
    "In logistic regression, the cost function (or loss function) is used to measure the difference between the predicted values and the actual values of the target variable. The most commonly used cost function for logistic regression is the logistic loss function, also known as the binary cross-entropy loss. The logistic loss function for a single training example is defined as:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    ",\n",
    "�\n",
    "^\n",
    ")\n",
    "=\n",
    "−\n",
    "[\n",
    "�\n",
    "log\n",
    "⁡\n",
    "(\n",
    "�\n",
    "^\n",
    ")\n",
    "+\n",
    "(\n",
    "1\n",
    "−\n",
    "�\n",
    ")\n",
    "log\n",
    "⁡\n",
    "(\n",
    "1\n",
    "−\n",
    "�\n",
    "^\n",
    ")\n",
    "]\n",
    "J(y, \n",
    "y\n",
    "^\n",
    "​\n",
    " )=−[ylog( \n",
    "y\n",
    "^\n",
    "​\n",
    " )+(1−y)log(1− \n",
    "y\n",
    "^\n",
    "​\n",
    " )]\n",
    "\n",
    "where:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    ",\n",
    "�\n",
    "^\n",
    ")\n",
    "J(y, \n",
    "y\n",
    "^\n",
    "​\n",
    " ) is the logistic loss for a single training example.\n",
    "�\n",
    "y is the actual label (0 or 1).\n",
    "�\n",
    "^\n",
    "y\n",
    "^\n",
    "​\n",
    "  is the predicted probability that \n",
    "�\n",
    "=\n",
    "1\n",
    "y=1.\n",
    "For the entire dataset, the logistic loss is often expressed as the average over all training examples:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "−\n",
    "1\n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "[\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "log\n",
    "⁡\n",
    "(\n",
    "�\n",
    "^\n",
    "(\n",
    "�\n",
    ")\n",
    ")\n",
    "+\n",
    "(\n",
    "1\n",
    "−\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    ")\n",
    "log\n",
    "⁡\n",
    "(\n",
    "1\n",
    "−\n",
    "�\n",
    "^\n",
    "(\n",
    "�\n",
    ")\n",
    ")\n",
    "]\n",
    "J(θ)=− \n",
    "m\n",
    "1\n",
    "​\n",
    " ∑ \n",
    "i=1\n",
    "m\n",
    "​\n",
    " [y \n",
    "(i)\n",
    " log( \n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "(i)\n",
    " )+(1−y \n",
    "(i)\n",
    " )log(1− \n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "(i)\n",
    " )]\n",
    "\n",
    "where:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "J(θ) is the overall logistic loss for the entire dataset.\n",
    "�\n",
    "m is the number of training examples.\n",
    "�\n",
    "θ represents the parameters (coefficients) of the logistic regression model.\n",
    "The goal during training is to find the values of \n",
    "�\n",
    "θ that minimize this cost function.\n",
    "\n",
    "Optimization:\n",
    "Logistic regression is typically optimized using iterative optimization algorithms, with the most common one being gradient descent. The idea is to update the parameters iteratively in the opposite direction of the gradient of the cost function with respect to the parameters. The gradient descent update rule for logistic regression is:\n",
    "\n",
    "�\n",
    "�\n",
    ":\n",
    "=\n",
    "�\n",
    "�\n",
    "−\n",
    "�\n",
    "1\n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "(\n",
    "�\n",
    "^\n",
    "(\n",
    "�\n",
    ")\n",
    "−\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    ")\n",
    "�\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "θ \n",
    "j\n",
    "​\n",
    " :=θ \n",
    "j\n",
    "​\n",
    " −α \n",
    "m\n",
    "1\n",
    "​\n",
    " ∑ \n",
    "i=1\n",
    "m\n",
    "​\n",
    " ( \n",
    "y\n",
    "^\n",
    "​\n",
    "  \n",
    "(i)\n",
    " −y \n",
    "(i)\n",
    " )x \n",
    "j\n",
    "(i)\n",
    "​\n",
    " \n",
    "\n",
    "where:\n",
    "\n",
    "�\n",
    "�\n",
    "θ \n",
    "j\n",
    "​\n",
    "  is the \n",
    "�\n",
    "j-th parameter (weight) of the model.\n",
    "�\n",
    "α is the learning rate.\n",
    "�\n",
    "m is the number of training examples.\n",
    "�\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "x \n",
    "j\n",
    "(i)\n",
    "​\n",
    "  is the \n",
    "�\n",
    "j-th feature of the \n",
    "�\n",
    "i-th training example.\n",
    "This process is repeated until convergence, where the cost function reaches a minimum or a sufficiently low value. There are variations of gradient descent, such as stochastic gradient descent (SGD) and mini-batch gradient descent, which use subsets of the training data to update parameters in each iteration.\n",
    "\n",
    "The goal of the optimization process is to find the set of parameters \n",
    "�\n",
    "θ that minimizes the logistic loss function, making the predicted probabilities as close as possible to the actual labels.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bd660c-6c49-43ce-a310-4f65599a9c6c",
   "metadata": {},
   "source": [
    "## Question-3 :Explain the concept of regularization in logistic regression and how it helps prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969d21fd-a65e-48ed-8038-b09b32fb5f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularization is a technique used to prevent overfitting in machine learning models, including logistic regression. The idea behind regularization is to add a penalty term to the cost function that discourages the model from fitting the training data too closely. This penalty term is based on the magnitude of the model parameters (weights).\n",
    "\n",
    "In logistic regression, two common types of regularization are L1 regularization (Lasso) and L2 regularization (Ridge). \n",
    "The regularization term is added to the original cost function, penalizing large values of the parameters. The choice between L1 and L2 regularization depends on the characteristics of the problem and the desired properties of the model.\n",
    "\n",
    "How Regularization Helps Prevent Overfitting:\n",
    "\n",
    "Penalizing Large Coefficients:\n",
    "\n",
    "Regularization penalizes large values of the model parameters by adding a term that depends on the square of the parameters (L2 regularization) or the absolute value of the parameters (L1 regularization).\n",
    "This discourages the model from assigning too much importance to any one feature, preventing it from fitting noise in the training data.\n",
    "Simplifying the Model:\n",
    "\n",
    "The regularization term encourages the model to be simpler by favoring smaller parameter values.\n",
    "Simpler models are less likely to overfit the training data and are more likely to generalize well to new, unseen data.\n",
    "Feature Selection (L1 Regularization):\n",
    "\n",
    "L1 regularization has the additional property of promoting sparsity in the model.\n",
    "It tends to drive some feature weights to exactly zero, effectively performing feature selection by excluding irrelevant features.\n",
    "Controlling Model Complexity:\n",
    "\n",
    "By controlling the strength of regularization with the hyperparameter \n",
    "�\n",
    "λ, you can find a balance between fitting the training data and keeping the model parameters in check.\n",
    "Larger values of \n",
    "�\n",
    "λ result in stronger regularization and a simpler model.\n",
    "Reducing Sensitivity to Outliers:\n",
    "\n",
    "Regularization can reduce the model's sensitivity to outliers in the training data, making the model more robust.\n",
    "In summary, regularization in logistic regression helps prevent overfitting by penalizing large parameter values, promoting model simplicity, and controlling the trade-off between fitting the training data and generalizing to new data. The choice of regularization type (L1 or L2) and the regularization parameter \n",
    "�\n",
    "λ are important considerations during model training.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820364f1-6308-43c0-9c56-1a51429da1fa",
   "metadata": {},
   "source": [
    "## Question-4 :What is the ROC curve, and how is it used to evaluate the performance of the logistic regression model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f3adfa-6224-4228-94b2-a239a177216e",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Receiver Operating Characteristic (ROC) curve is a graphical representation used to evaluate the performance of a binary classification model, such as logistic regression. It visualizes the trade-off between the true positive rate (sensitivity) and the false positive rate across different classification thresholds. The ROC curve is particularly useful when dealing with imbalanced datasets.\n",
    "\n",
    "Here are the key components of an ROC curve:\n",
    "\n",
    "True Positive Rate (Sensitivity or Recall):\n",
    "\n",
    "The true positive rate is the proportion of actual positive instances correctly predicted by the model.\n",
    "True Positive Rate (TPR)\n",
    "=\n",
    "True Positives\n",
    "True Positives + False Negatives\n",
    "True Positive Rate (TPR)= \n",
    "True Positives + False Negatives\n",
    "True Positives\n",
    "​\n",
    " \n",
    "False Positive Rate:\n",
    "\n",
    "The false positive rate is the proportion of actual negative instances incorrectly predicted as positive by the model.\n",
    "False Positive Rate (FPR)\n",
    "=\n",
    "False Positives\n",
    "False Positives + True Negatives\n",
    "False Positive Rate (FPR)= \n",
    "False Positives + True Negatives\n",
    "False Positives\n",
    "​\n",
    " \n",
    "Thresholds:\n",
    "\n",
    "The ROC curve is created by plotting the true positive rate against the false positive rate at various classification thresholds.\n",
    "Each point on the curve corresponds to a specific threshold used to classify instances as positive or negative.\n",
    "Area Under the Curve (AUC):\n",
    "\n",
    "The Area Under the ROC Curve (AUC) is a summary measure of the classifier's performance. It quantifies the overall ability of the model to discriminate between positive and negative instances.\n",
    "AUC values range from 0 to 1, where higher values indicate better performance. A model with an AUC of 0.5 suggests random performance, while an AUC of 1 indicates perfect discrimination.\n",
    "How to Interpret the ROC Curve:\n",
    "Top-Left Corner (0,1): This point represents a perfect classifier that achieves a true positive rate of 1 (100%) and a false positive rate of 0 (0%).\n",
    "\n",
    "Bottom-Right Corner (1,0): This point corresponds to a classifier that predicts all instances as negative, achieving a true positive rate of 0 (0%) and a false positive rate of 0 (0%).\n",
    "\n",
    "Diagonal Line (Random Guessing): The diagonal line (from the bottom-left to the top-right) represents random guessing, with an AUC of 0.5.\n",
    "\n",
    "Above Diagonal Line: Points above the diagonal line indicate better-than-random performance.\n",
    "\n",
    "Using ROC Curve for Logistic Regression:\n",
    "Model Comparison: ROC curves are useful for comparing the performance of different models. The model with a higher AUC is generally considered better.\n",
    "\n",
    "Optimal Threshold Selection: ROC curves help in selecting an optimal classification threshold based on the specific requirements of the application. You can choose a threshold that balances sensitivity and specificity according to the desired trade-off.\n",
    "\n",
    "Performance Assessment: The visual representation of the ROC curve provides insights into the model's ability to discriminate between positive and negative instances across a range of thresholds.\n",
    "\n",
    "Sensitivity and Specificity: Depending on the problem, you might prioritize sensitivity (true positive rate) over specificity (true negative rate) or vice versa. The ROC curve allows you to visualize this trade-off.\n",
    "\n",
    "In summary, the ROC curve and AUC provide a comprehensive evaluation of the performance of a logistic regression model, capturing its ability to discriminate between positive and negative instances at different classification thresholds. The curve is a valuable tool for making informed decisions about model performance and threshold selection.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3075f50-a8ee-4ac1-a02e-241632be11a9",
   "metadata": {},
   "source": [
    "## Question-5 :What are some common techniques for feature selection in logistic regression? How do these techniques help improve the model's performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb05ae56-dabc-4b4b-b27f-9d9e9de08756",
   "metadata": {},
   "outputs": [],
   "source": [
    "Feature selection is a process of choosing a subset of relevant features from the original set of features to improve model performance, reduce overfitting, and enhance interpretability. In the context of logistic regression, where the goal is binary classification, effective feature selection is crucial. Here are some common techniques for feature selection in logistic regression:\n",
    "\n",
    "1. Correlation Analysis:\n",
    "Identify and remove highly correlated features. High correlation between two features can lead to multicollinearity, which might affect the stability of the logistic regression coefficients.\n",
    "2. Univariate Feature Selection:\n",
    "Evaluate the statistical significance of each feature individually using statistical tests like chi-squared test, ANOVA, or mutual information.\n",
    "Select features based on their p-values or scores.\n",
    "3. Recursive Feature Elimination (RFE):\n",
    "RFE is an iterative method that involves fitting the model multiple times and eliminating the least significant features in each iteration.\n",
    "The process continues until the desired number of features is reached.\n",
    "4. L1 Regularization (Lasso Regression):\n",
    "Apply L1 regularization to the logistic regression model.\n",
    "L1 regularization introduces a penalty term that encourages sparsity in the coefficients, effectively performing feature selection by driving some coefficients to zero.\n",
    "5. Tree-Based Methods:\n",
    "Decision tree-based algorithms (e.g., Random Forest, Gradient Boosting) can be used to evaluate feature importance.\n",
    "Features are ranked based on how frequently they are used for decision-making in the ensemble of trees.\n",
    "6. Information Gain or Gain Ratio:\n",
    "Calculate the information gain or gain ratio for each feature based on entropy or Gini index.\n",
    "Select features that contribute the most to reducing uncertainty in the target variable.\n",
    "7. Variance Threshold:\n",
    "Remove features with low variance, assuming that features with little variation across instances may not provide much information.\n",
    "8. Sequential Feature Selection:\n",
    "Forward or backward feature selection involves iteratively adding or removing features based on model performance.\n",
    "The process continues until a certain criterion (e.g., cross-validated performance) is met.\n",
    "9. Principal Component Analysis (PCA):\n",
    "Transform the original features into a set of uncorrelated principal components.\n",
    "Retain a subset of principal components that capture most of the variability in the data.\n",
    "10. Mutual Information:\n",
    "sql\n",
    "Copy code\n",
    "- Assess the mutual information between each feature and the target variable.\n",
    "- Features with high mutual information are considered informative for predicting the target.\n",
    "How Feature Selection Improves Model Performance:\n",
    "Reduces Overfitting:\n",
    "\n",
    "By eliminating irrelevant or redundant features, the model is less likely to overfit the training data.\n",
    "Improves Model Interpretability:\n",
    "\n",
    "A simplified model with fewer features is easier to interpret and understand.\n",
    "Reduces Computational Complexity:\n",
    "\n",
    "Fewer features result in faster training times and less computational overhead.\n",
    "Enhances Generalization:\n",
    "\n",
    "Removing noise and irrelevant features helps the model generalize better to unseen data.\n",
    "Addresses Multicollinearity:\n",
    "\n",
    "Removing highly correlated features helps mitigate issues related to multicollinearity, which can destabilize coefficient estimates.\n",
    "Facilitates Faster Training and Inference:\n",
    "\n",
    "Training and making predictions with a reduced set of features can be computationally more efficient.\n",
    "It's important to note that the choice of feature selection technique depends on the specific characteristics of the dataset and the problem at hand. Combining multiple techniques or using domain knowledge to guide the selection process can often lead to more effective feature selection. Additionally, careful evaluation using validation sets or cross-validation is crucial to ensure that the selected features generalize well to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ee37da-2ae2-4ad5-99c5-9b857caa12b0",
   "metadata": {},
   "source": [
    "## Question-6 :How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing with class imbalance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f418837-b7d2-446a-a7ef-6d5ec935178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Handling imbalanced datasets in logistic regression is crucial to prevent the model from being biased towards the majority class and achieving poor performance on the minority class. Here are several strategies for dealing with class imbalance in logistic regression:\n",
    "\n",
    "1. Resampling Techniques:\n",
    "a. Oversampling the Minority Class:\n",
    "Increase the number of instances in the minority class by duplicating or generating synthetic examples (e.g., using techniques like SMOTE - Synthetic Minority Over-sampling Technique).\n",
    "This helps the model better learn the patterns in the minority class.\n",
    "b. Undersampling the Majority Class:\n",
    "Reduce the number of instances in the majority class by randomly removing samples.\n",
    "Undersampling aims to balance the class distribution in the training set.\n",
    "2. Weighted Logistic Regression:\n",
    "Assign different weights to the classes during model training.\n",
    "Give higher weights to the minority class to make misclassifications in the minority class more costly.\n",
    "3. Cost-Sensitive Learning:\n",
    "Introduce misclassification costs to penalize errors in the minority class more heavily.\n",
    "The cost parameter can be adjusted based on the severity of misclassifying instances from each class.\n",
    "4. Ensemble Methods:\n",
    "Use ensemble methods like Random Forest or Gradient Boosting.\n",
    "These methods can naturally handle imbalanced datasets and provide better generalization.\n",
    "5. Threshold Adjustment:\n",
    "Adjust the classification threshold based on the specific needs of the problem.\n",
    "If the cost of false positives or false negatives is unequal, adjusting the threshold can improve the balance.\n",
    "6. Generate Synthetic Samples:\n",
    "Use techniques like SMOTE (Synthetic Minority Over-sampling Technique) to generate synthetic samples for the minority class.\n",
    "SMOTE creates synthetic instances by interpolating between existing minority class instances.\n",
    "7. Anomaly Detection Techniques:\n",
    "Treat the minority class as the anomaly and use anomaly detection techniques.\n",
    "One-class SVM or isolation forests can be applied to detect instances that deviate from the majority class.\n",
    "8. Use Evaluation Metrics Sensitive to Imbalance:\n",
    "Instead of accuracy, use evaluation metrics that are sensitive to class imbalance, such as precision, recall, F1 score, or area under the ROC curve (AUC-ROC).\n",
    "9. Cross-Validation Strategies:\n",
    "Implement stratified k-fold cross-validation to ensure that each fold maintains the same class distribution as the original dataset.\n",
    "This helps in obtaining more robust performance estimates.\n",
    "10. Data Augmentation:\n",
    "vbnet\n",
    "Copy code\n",
    "- Augment the minority class by introducing variations in existing instances.\n",
    "- This can be applied to both numerical and categorical features.\n",
    "11. Different Algorithms:\n",
    "vbnet\n",
    "Copy code\n",
    "- Experiment with different algorithms that are inherently less sensitive to class imbalance, such as support vector machines (SVM) or decision trees.\n",
    "12. Combine Minority Class Instances:\n",
    "vbnet\n",
    "Copy code\n",
    "- For certain scenarios, combining instances of the minority class to create a composite class may be an option.\n",
    "13. Feature Engineering:\n",
    "diff\n",
    "Copy code\n",
    "- Carefully engineer features that provide better discrimination between classes.\n",
    "- Feature engineering can sometimes alleviate class imbalance issues.\n",
    "14. Use of Anomaly Detection Models:\n",
    "rust\n",
    "Copy code\n",
    "- Treat the minority class as an anomaly and use models designed for anomaly detection, such as one-class SVM or isolation forests.\n",
    "15. Evaluate on Multiple Metrics:\n",
    "vbnet\n",
    "Copy code\n",
    "- Consider evaluating the model on multiple metrics, including precision, recall, F1 score, and area under the ROC curve, to get a comprehensive understanding of performance.\n",
    "It's often beneficial to experiment with multiple strategies and combinations thereof to find the most effective approach for a specific imbalanced dataset and problem context. Additionally, considering the domain knowledge and understanding the implications of false positives and false negatives is crucial for selecting the appropriate strategy.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ad9cc7-14cf-4bad-ad79-29fd614a7c83",
   "metadata": {},
   "source": [
    "## Question-7 :Can you discuss some common issues and challenges that may arise when implementing logistic regression, and how they can be addressed? For example, what can be done if there is multicollinearity among the independent variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115420e5-73e7-49e6-ac5e-9fe62b97be1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

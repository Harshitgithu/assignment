{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b1a1b70-2209-4e2d-9bd7-ddf59abdfa49",
   "metadata": {},
   "source": [
    "## Question-1 :What is a projection and how is it used in PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec8cbb4-25e7-40f3-8cb2-757458b08298",
   "metadata": {},
   "outputs": [],
   "source": [
    "In the context of Principal Component Analysis (PCA), a projection refers to the transformation of high-dimensional data points onto a lower-dimensional subspace defined by the principal components. PCA is a dimensionality reduction technique that aims to capture the maximum variance in the data by identifying the principal components, which are linear combinations of the original features.\n",
    "\n",
    "Here's how the projection process works in PCA:\n",
    "\n",
    "Compute the Covariance Matrix:\n",
    "\n",
    "Begin by standardizing the data (subtracting the mean and dividing by the standard deviation) to ensure that all features are on a comparable scale.\n",
    "Calculate the covariance matrix, which represents the relationships between different features in the dataset.\n",
    "Compute Eigenvectors and Eigenvalues:\n",
    "\n",
    "The eigenvectors and eigenvalues of the covariance matrix are calculated. Eigenvectors represent the directions of maximum variance in the data, and eigenvalues indicate the magnitude of variance along these directions.\n",
    "Sort Eigenvectors:\n",
    "\n",
    "Sort the eigenvectors in descending order based on their corresponding eigenvalues. The eigenvector with the highest eigenvalue represents the direction of maximum variance.\n",
    "Select Principal Components:\n",
    "\n",
    "Choose the top k eigenvectors, where k is the desired dimensionality of the lower-dimensional subspace. These eigenvectors are the principal components.\n",
    "Create the Projection Matrix:\n",
    "\n",
    "Form a projection matrix by stacking the selected eigenvectors as columns. This matrix defines the transformation from the original high-dimensional space to the lower-dimensional subspace spanned by the principal components.\n",
    "Project Data Points:\n",
    "\n",
    "Multiply the original data matrix by the projection matrix to obtain the transformed data in the lower-dimensional space. Each row in the transformed matrix corresponds to a data point in the new subspace.\n",
    "The projection process allows you to represent the data using a reduced set of dimensions while retaining as much of the original variance as possible. The principal components, which form the basis of the lower-dimensional space, capture the directions of maximum variability in the data.\n",
    "\n",
    "The projected data retains the essential information about the relationships between data points, making it suitable for analysis, visualization, or as input for machine learning models with reduced dimensionality.\n",
    "\n",
    "In summary, a projection in PCA involves transforming high-dimensional data into a lower-dimensional subspace defined by the principal components, capturing the most significant variance in the data. This transformation facilitates dimensionality reduction while preserving essential information about the structure of the original data.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f15d7e-a5d0-4d81-94ef-e85b68c3d444",
   "metadata": {},
   "source": [
    "## Question-2 :How does the optimization problem in PCA work, and what is it trying to achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e1e823-fc3a-4825-8235-86dfbbc67211",
   "metadata": {},
   "outputs": [],
   "source": [
    "Principal Component Analysis (PCA) involves solving an optimization problem to find the principal components that maximize the variance in the data. The main goal of PCA is to project the original high-dimensional data onto a lower-dimensional subspace in such a way that the variance of the projected data is maximized. Here's an overview of the optimization problem in PCA and what it aims to achieve:\n",
    "\n",
    "Objective Function:\n",
    "In PCA, the optimization problem is typically formulated as finding a set of k orthonormal vectors (principal components) that maximize the variance of the projected data.\n",
    "\n",
    "Let X be the standardized data matrix with dimensions m x n, where m is the number of samples and n is the number of features.\n",
    "\n",
    "The objective is to find a matrix W (with dimensions n x k) of principal components, where k is the desired dimensionality of the lower-dimensional subspace.\n",
    "\n",
    "The optimization problem is to maximize the objective function:\n",
    "\n",
    "Maximize \n",
    "∥\n",
    "proj\n",
    "(\n",
    ")\n",
    "∥\n",
    "2\n",
    "Maximize  \n",
    "m\n",
    "1\n",
    "​\n",
    " ∑ \n",
    "i=1\n",
    "m\n",
    "​\n",
    " ∥proj \n",
    "W\n",
    "​\n",
    " (x \n",
    "i\n",
    "​\n",
    " )∥ \n",
    "2\n",
    " \n",
    "\n",
    "where \n",
    "proj\n",
    "\n",
    "proj \n",
    "W\n",
    "​\n",
    " (x \n",
    "i\n",
    "​\n",
    " ) is the projection of the data point\n",
    "x \n",
    "i\n",
    "​\n",
    "  onto the subspace defined by the columns of W.\n",
    "\n",
    "Constraint:\n",
    "The columns of W must be orthonormal, meaning that \n",
    "W \n",
    "T\n",
    " ⋅W=I, where I is the identity matrix.\n",
    "\n",
    "Solving the Optimization Problem:\n",
    "The solution to the optimization problem involves finding the k eigenvectors corresponding to the k largest eigenvalues of the covariance matrix of the standardized data matrix X.\n",
    "\n",
    "Compute the Covariance Matrix:\n",
    "\n",
    "Σ= \n",
    "m\n",
    "1\n",
    "​\n",
    " X \n",
    "T\n",
    " X\n",
    "Compute Eigenvalues and Eigenvectors:\n",
    "\n",
    "Solve the eigenvalue problem \n",
    "Σ\n",
    "Σv=λv, where \n",
    "λ is an eigenvalue and \n",
    "v is the corresponding eigenvector.\n",
    "Sort and Select Principal Components:\n",
    "\n",
    "Sort the eigenvectors in descending order based on their corresponding eigenvalues.\n",
    "Select the top k eigenvectors to form the matrix W.\n",
    "Projection Matrix:\n",
    "\n",
    "The projection matrix \n",
    "W is used to transform the original data matrix \n",
    "X into the lower-dimensional subspace: \n",
    "Z=X⋅W.\n",
    "The columns of the matrix \n",
    "W are the principal components, and the resulting matrix \n",
    "Z contains the projected data in the lower-dimensional subspace.\n",
    "\n",
    "Objective of PCA:\n",
    "The primary objective of PCA is to reduce the dimensionality of the data while retaining as much information as possible. By choosing the principal components that capture the most variance in the data, PCA helps uncover the underlying structure and relationships in the dataset. The lower-dimensional representation can be used for visualization, data compression, or as input for machine learning models with reduced feature space. The optimization problem ensures that the projection maximizes the variance, making PCA an effective technique for dimensionality reductio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8531ca44-ccc8-43dd-b684-8428dc07ab75",
   "metadata": {},
   "source": [
    "## Question-3 :What is the relationship between covariance matrices and PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a4eb4c-78f4-4eef-ae43-ab8ef6e89f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "The relationship between covariance matrices and Principal Component Analysis (PCA) is fundamental to understanding how PCA works. In PCA, the covariance matrix plays a central role in identifying the principal components, which represent the directions of maximum variance in the data. Let's explore this relationship:\n",
    "\n",
    "Covariance Matrix:\n",
    "\n",
    "The covariance matrix of a dataset is a symmetric matrix that quantifies the relationships between different features. For a standardized dataset \n",
    "�\n",
    "X with dimensions \n",
    "�\n",
    "×\n",
    "�\n",
    "m×n (where \n",
    "�\n",
    "m is the number of samples and \n",
    "�\n",
    "n is the number of features), the covariance matrix \n",
    "Σ\n",
    "Σ is computed as:\n",
    "Σ\n",
    "=\n",
    "1\n",
    "�\n",
    "�\n",
    "�\n",
    "�\n",
    "Σ= \n",
    "m\n",
    "1\n",
    "​\n",
    " X \n",
    "T\n",
    " X\n",
    "Eigendecomposition of Covariance Matrix:\n",
    "\n",
    "PCA involves the eigendecomposition of the covariance matrix. The eigendecomposition equation is given by:\n",
    "Σ\n",
    "�\n",
    "=\n",
    "�\n",
    "�\n",
    "Σv=λv\n",
    "where \n",
    "�\n",
    "λ is an eigenvalue, and \n",
    "�\n",
    "v is the corresponding eigenvector.\n",
    "Principal Components:\n",
    "\n",
    "The eigenvectors of the covariance matrix represent the principal components of the data. These are the directions in the feature space along which the data has the maximum variance. The eigenvalues associated with these eigenvectors indicate the magnitude of variance along each principal component.\n",
    "Covariance Explained by Principal Components:\n",
    "\n",
    "The eigenvalues represent the variance explained by each principal component. The larger the eigenvalue, the more variance the corresponding principal component captures in the data. The cumulative sum of eigenvalues gives the total variance in the dataset.\n",
    "Projection Matrix:\n",
    "\n",
    "The principal components are used to construct the projection matrix \n",
    "�\n",
    "W. This matrix is applied to the original data matrix \n",
    "�\n",
    "X to obtain the lower-dimensional representation \n",
    "�\n",
    "Z:\n",
    "�\n",
    "=\n",
    "�\n",
    "⋅\n",
    "�\n",
    "Z=X⋅W\n",
    "In summary, the steps involved in PCA related to the covariance matrix are as follows:\n",
    "\n",
    "Compute Covariance Matrix: Calculate the covariance matrix \n",
    "Σ\n",
    "Σ based on the standardized dataset.\n",
    "\n",
    "Eigendecomposition: Solve the eigendecomposition equation \n",
    "Σ\n",
    "�\n",
    "=\n",
    "�\n",
    "�\n",
    "Σv=λv to obtain the eigenvalues and eigenvectors.\n",
    "\n",
    "Sort Eigenvectors: Sort the eigenvectors in descending order based on their corresponding eigenvalues.\n",
    "\n",
    "Select Principal Components: Choose the top \n",
    "�\n",
    "k eigenvectors to form the projection matrix \n",
    "�\n",
    "W.\n",
    "\n",
    "Projection: Multiply the original data matrix \n",
    "�\n",
    "X by the projection matrix \n",
    "�\n",
    "W to obtain the lower-dimensional representation \n",
    "�\n",
    "Z.\n",
    "\n",
    "The covariance matrix and its eigendecomposition provide the necessary information for identifying the principal components, making PCA a powerful technique for dimensionality reduction and capturing the essential structure of the data.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033b796f-484a-47c1-8a59-8ad2acd2bfd6",
   "metadata": {},
   "source": [
    "## Question-4 :How does the choice of number of principal components impact the performance of PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c60af68-b2ad-4602-bc86-a4b6675aa865",
   "metadata": {},
   "outputs": [],
   "source": [
    "The choice of the number of principal components in PCA has a significant impact on the performance of the technique and, subsequently, the performance of any downstream analysis or machine learning model. The number of principal components determines the dimensionality of the lower-dimensional subspace onto which the original data is projected. Here's how the choice of the number of principal components can affect PCA performance:\n",
    "\n",
    "Explained Variance:\n",
    "\n",
    "Impact: The number of principal components chosen influences the proportion of total variance in the data that is retained in the lower-dimensional representation.\n",
    "Consideration: Selecting too few principal components may result in a substantial loss of information, while choosing too many may lead to excessive dimensionality reduction without significant reduction in information loss.\n",
    "Dimensionality Reduction:\n",
    "\n",
    "Impact: PCA is often used for dimensionality reduction. The number of principal components determines how much the data is compressed.\n",
    "Consideration: A balance needs to be struck. Too few principal components may not capture the essential features of the data, while too many may not provide significant reduction in dimensionality, defeating the purpose of the technique.\n",
    "Model Performance:\n",
    "\n",
    "Impact: In machine learning applications, the number of principal components used as input features can impact the performance of downstream models.\n",
    "Consideration: The optimal number of principal components is often determined through experimentation and validation, considering the trade-off between model simplicity and performance.\n",
    "Computational Efficiency:\n",
    "\n",
    "Impact: The computational cost of PCA is influenced by the number of principal components.\n",
    "Consideration: A higher number of principal components may require more computational resources and time. Choosing an appropriate number is essential for efficiency, especially in large datasets.\n",
    "Interpretability:\n",
    "\n",
    "Impact: As the number of principal components increases, interpretability of the lower-dimensional representation becomes more challenging.\n",
    "Consideration: A smaller number of principal components may result in a more interpretable representation, aiding in the understanding of the underlying structure in the data.\n",
    "Overfitting and Generalization:\n",
    "\n",
    "Impact: Using too many principal components can lead to overfitting, especially when the number of samples is limited.\n",
    "Consideration: Care must be taken to avoid overfitting by selecting an appropriate number of principal components that generalizes well to new, unseen data.\n",
    "To determine the optimal number of principal components, several methods can be employed, including:\n",
    "\n",
    "Explained Variance Threshold: Choose the number of components that explain a predetermined percentage of the total variance (e.g., 95% or 99%).\n",
    "\n",
    "Scree Plot or Elbow Method: Examine a scree plot of eigenvalues and choose the number of components at the \"elbow\" of the curve.\n",
    "\n",
    "Cross-Validation: Use cross-validation to evaluate model performance with different numbers of principal components and choose the configuration that provides the best generalization to unseen data.\n",
    "\n",
    "In summary, the choice of the number of principal components in PCA is a crucial decision that involves balancing information retention, dimensionality reduction, interpretability, and computational efficiency based on the specific goals of the analysis or modeling task.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bdef65-7072-457b-ba8a-61d6a97bf6bc",
   "metadata": {},
   "source": [
    "## Question-5 :How can PCA be used in feature selection, and what are the benefits of using it for this purpose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330635bd-1d46-4f4f-8a57-d1bf6877c684",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA can be used as a feature selection technique, although it is important to note that PCA is primarily a dimensionality reduction technique. However, in certain scenarios, the transformed principal components obtained through PCA can be used as a form of feature selection. Here's how PCA can be applied for feature selection and the benefits of using it for this purpose:\n",
    "\n",
    "1. **Transform Data into Principal Components:**\n",
    "   - Apply PCA to the original feature space to obtain the principal components. These components are linear combinations of the original features, capturing the directions of maximum variance in the data.\n",
    "\n",
    "2. **Select Top Principal Components:**\n",
    "   - Choose the top k principal components that account for a high percentage of the total variance in the data. This selection effectively represents a subset of the original features.\n",
    "\n",
    "3. **Transform Back to Feature Space:**\n",
    "   - Transform the selected principal components back to the original feature space. This results in a reduced set of features that are linear combinations of the original features.\n",
    "\n",
    "4. **Benefits of Using PCA for Feature Selection:**\n",
    "\n",
    "   - **Variance Retention:** PCA retains the directions of maximum variance in the data, and by selecting a subset of principal components, you can capture a significant portion of the variance with fewer features.\n",
    "\n",
    "   - **Dimensionality Reduction:** The selected principal components can serve as a lower-dimensional representation of the data, reducing the number of features while preserving essential information.\n",
    "\n",
    "   - **Collinearity Mitigation:** If the original features are highly correlated (collinear), PCA can decorrelate them, and the selected principal components are orthogonal. This can be beneficial in scenarios where multicollinearity is an issue.\n",
    "\n",
    "   - **Noise Reduction:** The higher-order principal components often capture noise or less significant variations in the data. By selecting a subset of principal components, you focus on the most informative components and reduce the impact of noise.\n",
    "\n",
    "   - **Simplification of Models:** Using a reduced set of features can lead to simpler and more interpretable models. This is particularly valuable in situations where model interpretability is crucial.\n",
    "\n",
    "   - **Computational Efficiency:** Working with a reduced set of features can improve computational efficiency, as models may require less time and resources to train and make predictions.\n",
    "\n",
    "   - **Overfitting Mitigation:** By reducing the dimensionality of the feature space, PCA can help mitigate overfitting, especially when the number of features is large compared to the number of samples.\n",
    "\n",
    "   - **Data Visualization:** If the dimensionality reduction is substantial, the transformed data can be visualized more easily, aiding in exploratory data analysis.\n",
    "\n",
    "   - **Preprocessing for Downstream Models:** The transformed features obtained through PCA can be used as input for downstream machine learning models, serving as a preprocessing step that simplifies the modeling process.\n",
    "\n",
    "It's important to note that while PCA provides benefits in terms of dimensionality reduction and feature selection, it may not always be the best choice for every scenario. The interpretability of the transformed features might be a concern, and other feature selection techniques that prioritize interpretability might be preferred in some cases. Additionally, the choice of the number of principal components is crucial and should be determined based on the specific goals and requirements of the analysis or modeling task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795aba9c-e79f-4f4f-8da3-18ef13a5d96b",
   "metadata": {},
   "source": [
    "## Question-6 :What are some common applications of PCA in data science and machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3f5c39-fa6c-4584-97ed-b623462a6f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Principal Component Analysis (PCA) is a versatile technique widely used in various applications across data science and machine learning. Here are some common applications of PCA:\n",
    "\n",
    "Dimensionality Reduction:\n",
    "\n",
    "Application: PCA is primarily employed for dimensionality reduction by transforming high-dimensional data into a lower-dimensional space while retaining the most important information.\n",
    "Benefits: Reducing dimensionality simplifies the data, speeds up computations, and can improve the performance of machine learning models.\n",
    "Feature Extraction:\n",
    "\n",
    "Application: PCA is used to extract a set of uncorrelated features (principal components) that capture the most significant variance in the data.\n",
    "Benefits: Extracted features can be more informative and less correlated than the original features, leading to improved model performance.\n",
    "Data Visualization:\n",
    "\n",
    "Application: PCA is applied for data visualization by projecting data into a lower-dimensional space, making it easier to plot and analyze.\n",
    "Benefits: Visualization aids in understanding data patterns, identifying clusters, and revealing relationships between data points.\n",
    "Noise Reduction:\n",
    "\n",
    "Application: PCA helps in reducing the impact of noise and irrelevant variations in the data by focusing on the dominant patterns.\n",
    "Benefits: Models trained on denoised data may generalize better to new, unseen instances.\n",
    "Clustering Analysis:\n",
    "\n",
    "Application: PCA is used as a preprocessing step in clustering algorithms to reduce dimensionality and improve the efficiency of clustering.\n",
    "Benefits: Clustering on a lower-dimensional space often leads to more accurate and efficient cluster assignments.\n",
    "Image Compression:\n",
    "\n",
    "Application: PCA is employed for image compression by representing images using a reduced set of principal components.\n",
    "Benefits: Compression reduces storage requirements while preserving essential image features, allowing for more efficient transmission and processing.\n",
    "Facial Recognition:\n",
    "\n",
    "Application: PCA is applied to extract facial features for facial recognition systems.\n",
    "Benefits: By reducing dimensionality, PCA can enhance the efficiency and accuracy of facial recognition algorithms.\n",
    "Anomaly Detection:\n",
    "\n",
    "Application: PCA is utilized for detecting anomalies or outliers in datasets.\n",
    "Benefits: By capturing the normal variation in the data, anomalies can be identified as deviations from the learned patterns.\n",
    "Bioinformatics:\n",
    "\n",
    "Application: PCA is used in bioinformatics to analyze high-dimensional biological data, such as gene expression profiles.\n",
    "Benefits: Identifying key components in biological datasets helps in understanding underlying patterns and relationships.\n",
    "Chemometrics:\n",
    "\n",
    "Application: PCA is employed in chemometrics for analyzing spectroscopic data and chemical compositions.\n",
    "Benefits: It helps identify relevant features in complex chemical datasets and aids in quality control and process optimization.\n",
    "Speech Recognition:\n",
    "\n",
    "Application: PCA is applied in speech recognition to reduce the dimensionality of feature vectors.\n",
    "Benefits: Lower-dimensional representations simplify the processing of speech data, improving the efficiency of recognition systems.\n",
    "Collinearity Removal:\n",
    "\n",
    "Application: PCA is used to address multicollinearity in regression analysis by transforming correlated features into uncorrelated principal components.\n",
    "Benefits: It improves the stability and interpretability of regression models.\n",
    "In summary, PCA is a versatile tool with broad applications, including dimensionality reduction, feature extraction, data visualization, noise reduction, and various domain-specific analyses in fields such as image processing, biology, chemistry, and more. Its ability to capture dominant patterns in high-dimensional data makes it a valuable technique in many data science and machine learning tasks.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8a7866-1f7e-4037-a5ce-8901d17b75a3",
   "metadata": {},
   "source": [
    "## Question-7 :What is the relationship between spread and variance in PCA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c105ea-ce5a-46cb-b0a6-1f4539852715",
   "metadata": {},
   "outputs": [],
   "source": [
    "In the context of Principal Component Analysis (PCA), \"spread\" and \"variance\" are related concepts, and understanding their connection is essential for grasping the key principles of PCA. Let's explore the relationship between spread and variance in PCA:\n",
    "\n",
    "Variance:\n",
    "\n",
    "Definition: In PCA, variance is a measure of the amount of information, or spread, along a particular axis or direction in the data.\n",
    "Significance: Principal components are chosen to maximize the variance along each component, meaning that the first principal component captures the direction of maximum variance, the second principal component captures the direction of the second-highest variance (orthogonal to the first), and so on.\n",
    "Spread:\n",
    "\n",
    "Definition: In the context of PCA, \"spread\" refers to the distribution or dispersion of data points along a particular axis or direction.\n",
    "Significance: The spread of data points along the principal components indicates how much variability or information is captured in each direction.\n",
    "Mathematical Relationship:\n",
    "\n",
    "Explanation: Variance is a quantitative measure of spread, and in the context of PCA, maximizing variance is equivalent to maximizing spread along each principal component.\n",
    "Equation: For a given principal component \n",
    "�\n",
    "v with associated eigenvalue \n",
    "�\n",
    "λ, the variance along that component is proportional to \n",
    "�\n",
    "λ. The larger the eigenvalue, the more spread or variance is captured along the corresponding principal component.\n",
    "Eigenvalues and Spread:\n",
    "\n",
    "Explanation: In PCA, the eigenvalues of the covariance matrix represent the amount of variance (spread) along each principal component.\n",
    "Mathematical Relationship: If \n",
    "�\n",
    "1\n",
    ",\n",
    "�\n",
    "2\n",
    ",\n",
    "…\n",
    ",\n",
    "�\n",
    "�\n",
    "λ \n",
    "1\n",
    "​\n",
    " ,λ \n",
    "2\n",
    "​\n",
    " ,…,λ \n",
    "n\n",
    "​\n",
    "  are the eigenvalues corresponding to the principal components, the spread along the \n",
    "�\n",
    "i-th principal component is proportional to \n",
    "�\n",
    "�\n",
    "λ \n",
    "i\n",
    "​\n",
    " .\n",
    "In summary, the relationship between spread and variance in PCA is that maximizing variance along each principal component effectively maximizes the spread of data points in the corresponding direction. The eigenvalues associated with the principal components quantify the amount of variance (spread) captured along each axis, and the principal components are selected to align with these directions to capture the most information in the data.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e379c9-7582-4789-ada6-f8246b5530d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Question-8 :Q8. How does PCA use the spread and variance of the data to identify principal components?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

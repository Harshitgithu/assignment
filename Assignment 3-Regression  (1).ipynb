{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83c10f1a-fdf8-4650-84e3-fa1b56477104",
   "metadata": {},
   "source": [
    "## Question-1 :What is Ridge Regression, and how does it differ from ordinary least squares regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73b5c3b4-11cd-4e00-86fb-4f1bebb3b6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ridge regression is a model tuning method that is used to analyse any data that suffers from multicollinearity. This method performs L2 regularization. When the issue of multicollinearity occurs, least-squares are unbiased, and variances are large, this results in predicted values being far away from the actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fc7353-05f5-4be0-aa85-a6466fe11e9a",
   "metadata": {},
   "source": [
    "## Question-2 :What are the assumptions of Ridge Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be488eda-27ae-4493-8597-b5192ebef297",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The assumptions of ridge regression are the same as that of linear regression: linearity, constant variance, and independence. However, as ridge regression does not provide confidence limits, the distribution of errors to be normal need not be assumed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525d15e4-0fe4-446f-bb72-ff61bb315a8f",
   "metadata": {},
   "source": [
    "## Question-3 :How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e03a163a-f856-4098-aa42-45890b6a5e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Selecting a good value for λ is critical. When λ=0, the penalty term has no effect, and ridge regression will produce the classical least square coefficients. However, as λ increases to infinite, the impact of the shrinkage penalty grows, and the ridge regression coefficients will get close zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdc61ff-b945-4c47-a805-e5ecf753c6a4",
   "metadata": {},
   "source": [
    "## Question-4 :Can Ridge Regression be used for feature selection? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "741ac06c-70fd-423a-842a-f8658c3e2a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "## we could see ridge regression as doing the feature 'selection' in a nuanced way by reducing the size of the coefficients instead of setting them equal to zero. we could elliminate the features with the smaller coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b405fa-8de0-4643-a77f-3bb3b1cd54d9",
   "metadata": {},
   "source": [
    "## Question-5 :How does the Ridge Regression model perform in the presence of multicollinearity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e63ac965-a666-403a-a812-a0982cc01101",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multicollinearity happens when predictor variables exhibit a correlation among themselves. Ridge regression aims at reducing the standard error by adding some bias in the estimates of the regression. The reduction of the standard error in regression estimates significantly increases the reliability of the estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a32bd0-17cd-4193-9a21-ba12266a9d46",
   "metadata": {},
   "source": [
    "## Question-6 :Can Ridge Regression handle both categorical and continuous independent variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7beeb508-dfbd-408b-ab91-67ee1256a86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## f you have categorical data (or mixed numeric and categorical) and you are going to create a linear ridge regression model, you can use one-hot encoding on categorical variables that have three or more possible values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da6dfed-0a36-4265-ac3d-e18f7a084af6",
   "metadata": {},
   "source": [
    "## Question-7 How do you interpret the coefficients of Ridge Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebfa9008-f704-421a-abda-e45f0be7a95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The ridge coefficients are a reduced factor of the simple linear regression coefficients and thus never attain zero values but very small values. The lasso coefficients become zero in a certain range and are reduced by a constant factor, which explains their low magnitude in comparison to the ridge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c4eebd-1c61-467b-87a3-c498090af38e",
   "metadata": {},
   "source": [
    "## Question-8: Can Ridge Regression be used for time-series data analysis? If yes, how?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54ad3f56-cad6-4ef0-a3ee-7e596e81ef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Time series regression helps you understand the relationship between variables over time and forecast future values of the dependent variable. Some common application examples of time series regression include: predicting stock prices based on economic indicators. forecasting electricity demand based on weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e54134-3978-4476-9e10-a1cc09ab9122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

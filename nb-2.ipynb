{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67f1e98f-b62f-461f-b8a8-0b8655954f67",
   "metadata": {},
   "source": [
    "## Question-1 :A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cb24f6-d6cc-45fa-891e-de631a3ead88",
   "metadata": {},
   "outputs": [],
   "source": [
    "To find the probability that an employee is a smoker given that they use the health insurance plan, we can use Bayes' Theorem. Let's define the events:\n",
    "\n",
    "Event\n",
    "A: An employee is a smoker.\n",
    "Event\n",
    "B: An employee uses the health insurance plan.\n",
    "The probability of using the health insurance plan \n",
    "P(B) is the overall probability of employees using the plan, which is given as 70% or 0.7\n",
    "=\n",
    "0.7\n",
    "P(B)=0.7\n",
    "\n",
    "The probability of being a smoker given that an employee uses the health insurance \n",
    "P(A∣B) is given as 40% or 0.4\n",
    "=\n",
    "0.4\n",
    "P(A∣B)=0.4\n",
    "\n",
    "Now, we can use Bayes' Theorem to find the probability of being a smoker given that an employee uses the health insurance plan:\n",
    "\n",
    "P(A∣B)= \n",
    "P(B)\n",
    "P(B∣A)⋅P(A)\n",
    "​\n",
    " \n",
    "\n",
    "However, we need \n",
    "P(B∣A), which is the probability of using the health insurance plan given that an employee is a smoker. This information is not provided directly, but we can infer it using the fact that all employees who use the plan are either smokers or non-smok\n",
    "=\n",
    "1\n",
    "P(B∣A)=1\n",
    "\n",
    "Now we can substitute the values into Bayes' Theorem:\n",
    "\n",
    "P(A∣B)= \n",
    "P(B)\n",
    "1⋅P(A)\n",
    "​\n",
    " \n",
    "=\n",
    "1\n",
    "⋅\n",
    "0.4\n",
    "0.7\n",
    "P(A∣B)= \n",
    "0.7\n",
    "1⋅\n",
    "0.4\n",
    "≈\n",
    "0.5714\n",
    "P(A∣B)≈0.5714\n",
    "\n",
    "So, the probability that an employee is a smoker given that he/she uses the health insurance plan is approximately 0.5714 or 57.14%.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81884858-31ce-45c5-8821-2200f334743f",
   "metadata": {},
   "source": [
    "## Question-2 :What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260c46e0-ffeb-4d80-bc8e-1bc9f125ddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "The primary difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the nature of the features they are designed to handle:\n",
    "\n",
    "Bernoulli Naive Bayes:\n",
    "\n",
    "Nature of Features: Suitable for binary feature data (features that can take on only two values, typically 0 or 1).\n",
    "Example Applications:\n",
    "Document classification based on the presence or absence of specific words.\n",
    "Sentiment analysis where features indicate the presence or absence of certain words or phrases.\n",
    "Probability Distribution: Assumes a Bernoulli distribution for each feature.\n",
    "Multinomial Naive Bayes:\n",
    "\n",
    "Nature of Features: Suitable for discrete feature data where features represent counts or frequencies (non-negative integers).\n",
    "Example Applications:\n",
    "Text classification tasks, where features represent word counts or frequencies in a document.\n",
    "Document classification based on the frequency of terms.\n",
    "Probability Distribution: Assumes a Multinomial distribution for each feature.\n",
    "Key Differences:\n",
    "Feature Representation:\n",
    "\n",
    "Bernoulli Naive Bayes is appropriate when features are binary, representing the presence or absence of specific attributes.\n",
    "Multinomial Naive Bayes is used when features are discrete and represent counts or frequencies.\n",
    "Probability Distributions:\n",
    "\n",
    "Bernoulli Naive Bayes assumes a Bernoulli distribution for each feature, modeling the probability of observing a binary outcome.\n",
    "Multinomial Naive Bayes assumes a Multinomial distribution for each feature, modeling the probability of observing counts or frequencies in multiple categories.\n",
    "Example Applications:\n",
    "\n",
    "Bernoulli Naive Bayes is often applied to problems where the presence or absence of features is more relevant (e.g., document classification based on word presence).\n",
    "Multinomial Naive Bayes is commonly used in text classification tasks where the frequency of terms in a document is important.\n",
    "Handling Zeros:\n",
    "\n",
    "In Bernoulli Naive Bayes, the absence of a feature is explicitly considered, and zeros play a meaningful role.\n",
    "In Multinomial Naive Bayes, zeros might be handled differently, especially when dealing with term frequencies. Techniques like Laplace smoothing (additive smoothing) are commonly used to handle zero-frequency issues.\n",
    "In summary, the choice between Bernoulli Naive Bayes and Multinomial Naive Bayes depends on the nature of the features in the dataset. If features are binary (e.g., presence or absence), Bernoulli Naive Bayes may be suitable. If features represent counts or frequencies, Multinomial Naive Bayes is often a better choice.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56539237-d1b5-489f-adf6-266480203606",
   "metadata": {},
   "source": [
    "## Question-3 :How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec8aca9-1a6b-4694-9de3-eb6bfcb80f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bernoulli Naive Bayes, like other Naive Bayes classifiers, typically assumes that missing values are informative and contributes to the evidence in favor of both the presence and absence of a feature. In the context of Bernoulli Naive Bayes, where features are binary (taking values 0 or 1), the handling of missing values can be approached as follows:\n",
    "\n",
    "Presence of a Feature (1):\n",
    "\n",
    "If a feature is missing for a particular instance, it can be interpreted as evidence against the presence of the feature (assuming the feature is binary).\n",
    "In the absence of direct information, the classifier may treat missing values as equivalent to the feature being absent.\n",
    "Absence of a Feature (0):\n",
    "\n",
    "Conversely, if a feature is present, it contributes evidence in favor of the presence of the feature.\n",
    "If the feature is binary and takes the value 1, the missing value is treated as if the feature is present.\n",
    "Handling in Practice:\n",
    "\n",
    "The actual handling of missing values in Bernoulli Naive Bayes can depend on the specific implementation or library being used.\n",
    "Some implementations may automatically handle missing values by considering them as a separate category or by applying some form of imputation.\n",
    "Imputation methods might involve replacing missing values with a default value (e.g., 0) or using more sophisticated techniques, such as mean imputation.\n",
    "Consideration of Missing Values during Training:\n",
    "\n",
    "During the training phase, the presence or absence of a feature for instances with missing values is taken into account when estimating probabilities.\n",
    "The probabilities are adjusted based on the available information, and the model learns from instances with and without missing values.\n",
    "Practical Recommendation:\n",
    "\n",
    "When working with Bernoulli Naive Bayes and encountering missing values, it's essential to consider the characteristics of the dataset and the implications of treating missing values as evidence for or against the presence of a feature.\n",
    "If the missing values are believed to be informative, their treatment should align with the assumptions of the classifier.\n",
    "It's important to note that handling missing values in machine learning models, including Naive Bayes classifiers, can have a significant impact on model performance. Therefore, the specific approach to handling missing values should be chosen carefully based on the characteristics of the data and the problem at hand.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddaf164-3bf9-4a57-8771-5b943dd78cc6",
   "metadata": {},
   "source": [
    "## Question-4 :Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53352f2a-650b-4d98-94cb-0e9ee74d57c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. The Gaussian Naive Bayes classifier is an extension of the Naive Bayes algorithm that assumes that the features follow a Gaussian (normal) distribution. It is commonly used for continuous-valued features.\n",
    "\n",
    "In the context of multi-class classification, the Gaussian Naive Bayes classifier can be extended to handle more than two classes. The approach involves estimating the parameters of the Gaussian distribution (mean and variance) for each class, and then using Bayes' Theorem to calculate the posterior probabilities for each class given a set of feature values.\n",
    "\n",
    "Here are the key steps involved in using Gaussian Naive Bayes for multi-class classification:\n",
    "\n",
    "Parameter Estimation:\n",
    "\n",
    "For each class \n",
    "i, estimate the mean (\n",
    "μ \n",
    "i\n",
    "​\n",
    " ) and variance (\n",
    "2\n",
    "σ \n",
    "i\n",
    "2\n",
    "​\n",
    " ) of each feature in the training data. These estimates are used to model the Gaussian distribution for each class.\n",
    "Class Prior Probability:\n",
    "\n",
    "Calculate the prior probability of each class \n",
    "P(C \n",
    "i\n",
    "​\n",
    " )), which represents the probability of a randomly chosen instance belonging to class \n",
    "i.\n",
    "Likelihood Calculation:\n",
    "\n",
    "Given a set of feature values, calculate the likelihood of those values under the assumed Gaussian distribution for each class.\n",
    "Posterior Probability Calculation:\n",
    "\n",
    "Use Bayes' Theorem to calculate the posterior probability of each class given the feature values.\n",
    "Decision Rule:\n",
    "\n",
    "Assign the instance to the class with the highest posterior probability.\n",
    "The decision rule is typically implemented using the maximum a posteriori (MAP) estimation, which assigns the class that maximizes the posterior probability.\n",
    "\n",
    "Gaussian Naive Bayes is particularly suitable when dealing with continuous-valued features, and it can be a simple yet effective classifier for multi-class problems. It is important to note that the \"Naive\" assumption of independence among features is retained, which means that the model assumes that the features are conditionally independent given the class label. Despite this simplifying assumption, Gaussian Naive Bayes often performs well in practice, especially when the feature independence assumption is reasonable for the given problem.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c869fb-a921-4c7b-ba21-ea1bcd0432ff",
   "metadata": {},
   "source": [
    "## Question-5 :Assignment:\n",
    "## Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "## is spam or not based on several input features.\n",
    "## Implementation:\n",
    "## scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "## dataset. You should use the default hyperparameters for each classifier.\n",
    "## Results:\n",
    "## Report the following performance metrics for each classifier:\n",
    "## Accuracy\n",
    "## Precision\n",
    "## F1 score\n",
    "## Discussion:\n",
    "## Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "## the case? Are there any limitations of Naive Bayes that you observed?\n",
    "## Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bdc6f7-7b5f-42d7-b521-5cf04faf4f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the Spambase dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "names = [f\"X{i}\" for i in range(57)] + [\"label\"]\n",
    "data = pd.read_csv(url, names=names, header=None)\n",
    "\n",
    "# Split the data into features (X) and labels (y)\n",
    "X = data.drop(\"label\", axis=1)\n",
    "y = data[\"label\"]\n",
    "\n",
    "# Instantiate the Naive Bayes classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Perform 10-fold cross-validation and calculate performance metrics\n",
    "def evaluate_classifier(classifier, X, y):\n",
    "    accuracy = cross_val_score(classifier, X, y, cv=10, scoring='accuracy').mean()\n",
    "    precision = cross_val_score(classifier, X, y, cv=10, scoring='precision').mean()\n",
    "    recall = cross_val_score(classifier, X, y, cv=10, scoring='recall').mean()\n",
    "    f1 = cross_val_score(classifier, X, y, cv=10, scoring='f1').mean()\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Evaluate each classifier\n",
    "accuracy_bernoulli, precision_bernoulli, recall_bernoulli, f1_bernoulli = evaluate_classifier(bernoulli_nb, X, y)\n",
    "accuracy_multinomial, precision_multinomial, recall_multinomial, f1_multinomial = evaluate_classifier(multinomial_nb, X, y)\n",
    "accuracy_gaussian, precision_gaussian, recall_gaussian, f1_gaussian = evaluate_classifier(gaussian_nb, X, y)\n",
    "\n",
    "# Print the results\n",
    "print(\"Results for Bernoulli Naive Bayes:\")\n",
    "print(f\"Accuracy: {accuracy_bernoulli}\")\n",
    "print(f\"Precision: {precision_bernoulli}\")\n",
    "print(f\"Recall: {recall_bernoulli}\")\n",
    "print(f\"F1 Score: {f1_bernoulli}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Results for Multinomial Naive Bayes:\")\n",
    "print(f\"Accuracy: {accuracy_multinomial}\")\n",
    "print(f\"Precision: {precision_multinomial}\")\n",
    "print(f\"Recall: {recall_multinomial}\")\n",
    "print(f\"F1 Score: {f1_multinomial}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Results for Gaussian Naive Bayes:\")\n",
    "print(f\"Accuracy: {accuracy_gaussian}\")\n",
    "print(f\"Precision: {precision_gaussian}\")\n",
    "print(f\"Recall: {recall_gaussian}\")\n",
    "print(f\"F1 Score: {f1_gaussian}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1720d7eb-f918-4eac-800f-548d8fa359f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7436c3-0c76-4248-8abc-2d734f44fb58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

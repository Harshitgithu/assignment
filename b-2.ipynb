{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec901c2f-8cff-4723-a064-cd35000bb9d3",
   "metadata": {},
   "source": [
    "## Question-1 :What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b26621-95f6-4d1e-9ed3-47a35d39fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient Boosting Regression is a machine learning technique that falls under the broader category of ensemble learning. Specifically, it is a regression algorithm that builds a predictive model by combining the predictions of multiple weak learners, often decision trees. Gradient Boosting Regression is an extension of the more general Gradient Boosting framework, which can be applied to various types of tasks, including both regression and classification.\n",
    "\n",
    "Here's an overview of how Gradient Boosting Regression works:\n",
    "\n",
    "Initialization:\n",
    "\n",
    "The algorithm starts with an initial prediction, which is usually the mean (or median) of the target variable for regression problems.\n",
    "Sequential Model Building:\n",
    "\n",
    "Iteratively, a series of weak learners (typically decision trees) are added to the ensemble. Each new weak learner is trained to correct the errors made by the existing ensemble.\n",
    "Loss Function Optimization:\n",
    "\n",
    "Gradient Boosting Regression minimizes a specified loss function (e.g., mean squared error for regression tasks) by adjusting the predictions of the ensemble.\n",
    "In each iteration, the gradient of the loss with respect to the current predictions is computed. The weak learner is trained to fit the negative gradient, essentially modeling the residual errors.\n",
    "Learning Rate:\n",
    "\n",
    "A learning rate parameter is used to control the contribution of each weak learner to the overall ensemble. A lower learning rate requires more weak learners but can improve the model's generalization.\n",
    "Tree Constraints:\n",
    "\n",
    "Typically, weak learners are shallow decision trees to avoid overfitting. Constraints on tree depth and other regularization parameters are often applied.\n",
    "Combining Predictions:\n",
    "\n",
    "The predictions of all weak learners are combined to form the final prediction. This combination is usually done through a weighted sum of the individual predictions.\n",
    "The overall process of Gradient Boosting Regression is characterized by the sequential training of weak learners, where each new learner focuses on the mistakes made by the existing ensemble. The learning process is driven by the optimization of a specified loss function, and the final model becomes a weighted sum of the weak learners' predictions.\n",
    "\n",
    "Popular implementations of Gradient Boosting Regression include XGBoost, LightGBM, and scikit-learn's GradientBoostingRegressor. These frameworks often provide optimizations, parallelization, and additional features to enhance the efficiency and performance of Gradient Boosting Regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01321519-6d37-4b7d-8459-95f84e18b24e",
   "metadata": {},
   "source": [
    "## Question-2 :Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb7c854-5202-44be-be19-d110ff4a950f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.estimators = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize with the mean of the target variable\n",
    "        initial_prediction = np.mean(y)\n",
    "        self.estimators.append(initial_prediction)\n",
    "\n",
    "        # Iterate to train weak learners\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute residuals\n",
    "            residuals = y - self.predict(X)\n",
    "\n",
    "            # Train a weak learner (Decision Tree) on the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=3)\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # Update the ensemble with the weak learner's predictions\n",
    "            self.estimators.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predictions are the cumulative sum of weak learners' predictions\n",
    "        predictions = np.cumsum([self.learning_rate * tree.predict(X) for tree in self.estimators])\n",
    "        return predictions\n",
    "\n",
    "# Generate a small dataset for regression\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1)\n",
    "y = 2 * X.squeeze() + np.random.normal(scale=0.1, size=100)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the gradient boosting model\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred[-1])\n",
    "r2 = r2_score(y_test, y_pred[-1])\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd5331e-e8eb-4cc6-828b-155649da9fb2",
   "metadata": {},
   "source": [
    "## Question-3 :Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cea31f-40ec-478d-a84b-8dd51a784212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define a parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Create the gradient boosting model\n",
    "gb_model = GradientBoostingRegressor()\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=gb_model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_gb_model = GradientBoostingRegressor(**best_params)\n",
    "best_gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_best = best_gb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model with the best hyperparameters\n",
    "mse_best = mean_squared_error(y_test, y_pred_best[-1])\n",
    "r2_best = r2_score(y_test, y_pred_best[-1])\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Mean Squared Error:\", mse_best)\n",
    "print(\"Best R-squared:\", r2_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c22715-b2b0-4db0-8106-fba344dda582",
   "metadata": {},
   "source": [
    "## Question-4 :What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c348af-46f0-4311-b3b3-a3a502ac59f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "In the context of Gradient Boosting, a weak learner refers to a model that performs slightly better than random chance on a given task. The term \"weak\" is used to indicate that these learners are not highly expressive or complex individually, and their predictive performance is only slightly better than random guessing.\n",
    "\n",
    "Common examples of weak learners used in Gradient Boosting include shallow decision trees, often referred to as decision stumps. A decision stump is a tree with a single decision node and two leaf nodes. It makes predictions based on a simple rule, such as comparing the value of a single feature to a threshold.\n",
    "\n",
    "The choice of weak learners is intentional in Gradient Boosting. The algorithm relies on the additive combination of many weak learners to create a strong ensemble model. Each weak learner is trained to correct the errors made by the ensemble up to that point. The iterative nature of Gradient Boosting allows it to adapt and improve with each additional weak learner.\n",
    "\n",
    "Key characteristics of weak learners in the context of Gradient Boosting:\n",
    "\n",
    "Low Complexity: Weak learners are typically simple models with low complexity, such as shallow decision trees or linear models. Their simplicity helps prevent overfitting.\n",
    "\n",
    "Low Predictive Power: Individually, weak learners may not provide highly accurate predictions on their own. However, when combined in an ensemble, their collective power contributes to improved model performance.\n",
    "\n",
    "Focus on Residuals: Weak learners are trained to predict the residuals (the differences between actual and predicted values) of the ensemble up to the current iteration. This focuses their learning on the errors made by the existing model.\n",
    "\n",
    "Complementary Weaknesses: Each weak learner may be specialized in capturing certain patterns or relationships in the data. The combination of diverse weak learners contributes to the ensemble's ability to model complex relationships.\n",
    "\n",
    "The concept of weak learners is fundamental to the success of Gradient Boosting algorithms. By iteratively adding weak learners and adjusting their contributions, the algorithm builds a strong predictive model capable of capturing intricate patterns and dependencies in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c85100-75a9-47f7-8ad2-cfcae332f78c",
   "metadata": {},
   "source": [
    "## Question- 5:What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1113e5a9-2ede-4925-ac2a-56d2f93bc439",
   "metadata": {},
   "outputs": [],
   "source": [
    "The intuition behind the Gradient Boosting algorithm can be understood through the analogy of a \"team of experts.\" Imagine you have a team of experts, each with their own strengths and weaknesses. The goal is to make accurate predictions, and each expert in the team focuses on correcting the mistakes made by the ensemble up to that point.\n",
    "\n",
    "Here's a step-by-step intuition behind how Gradient Boosting works:\n",
    "\n",
    "Initialization:\n",
    "\n",
    "The process begins with an initial prediction, often a simple one like the mean of the target variable for regression problems.\n",
    "First Weak Learner:\n",
    "\n",
    "The first weak learner is trained to predict the errors (residuals) between the initial prediction and the true labels in the training data.\n",
    "Building a Team of Experts:\n",
    "\n",
    "The weak learner's predictions are added to the initial prediction, and the ensemble's performance is assessed.\n",
    "A new weak learner is introduced to correct the errors made by the current ensemble. This learner focuses on the residuals from the combined predictions of the previous weak learners.\n",
    "Iterative Correction:\n",
    "\n",
    "The process is repeated iteratively. In each iteration, a new weak learner is introduced to the ensemble, trained to predict the residuals of the current ensemble.\n",
    "The weak learners are like experts in a team, each contributing their expertise to correct specific types of errors.\n",
    "Combining Predictions:\n",
    "\n",
    "The predictions of all weak learners are combined, often through a weighted sum, to form the final prediction of the Gradient Boosting model.\n",
    "The intuition is that each weak learner corrects the mistakes of the ensemble up to the current iteration. The weights assigned to each learner depend on their ability to minimize the errors. As more weak learners are added, the ensemble adjusts and refines its predictions, gradually improving overall accuracy.\n",
    "\n",
    "The term \"Gradient\" in Gradient Boosting refers to the optimization process. In each iteration, the algorithm minimizes the loss function by moving in the direction (gradient) that reduces the prediction errors. The learning rate controls the step size during optimization.\n",
    "\n",
    "In summary, the intuition behind Gradient Boosting involves building a team of weak learners (experts) that work collaboratively to improve predictions by focusing on correcting errors made by the ensemble up to the current point. This iterative correction process leads to a powerful and accurate predictive model.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1db50d5-fe23-41f7-9430-acae2abfd86a",
   "metadata": {},
   "source": [
    "## Question-6 :How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac7ff0d-1600-4241-81a3-db9ebf457a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential and additive manner. The process involves iteratively introducing weak learners (often decision trees) to the ensemble, with each new learner focusing on the errors made by the existing ensemble. Here's a step-by-step explanation of how the ensemble is built:\n",
    "\n",
    "Initialization:\n",
    "\n",
    "The process starts with an initial prediction, which is often a simple estimate like the mean of the target variable for regression problems.\n",
    "Compute Residuals:\n",
    "\n",
    "The first weak learner is trained on the original data to predict the difference (residuals) between the initial prediction and the true labels. These residuals represent the errors made by the initial prediction.\n",
    "Update Ensemble:\n",
    "\n",
    "The predictions of the first weak learner are added to the initial prediction to update the ensemble. The combined predictions now provide a better approximation of the true labels.\n",
    "Iterative Process:\n",
    "\n",
    "In subsequent iterations, new weak learners are introduced to the ensemble.\n",
    "Each weak learner is trained to predict the residuals of the current ensemble. This means it focuses on the errors made by the existing ensemble.\n",
    "Update Ensemble in Each Iteration:\n",
    "\n",
    "After each iteration, the predictions of the new weak learner are added to the ensemble, updating the overall prediction.\n",
    "Sequential Learning:\n",
    "\n",
    "The algorithm continues this process for a predefined number of iterations or until a certain level of performance is reached. Each weak learner is trained to correct the mistakes of the ensemble up to the current iteration.\n",
    "Combination of Predictions:\n",
    "\n",
    "The final prediction of the Gradient Boosting model is the sum of the predictions from all weak learners, each multiplied by a learning rate that controls the contribution of each learner to the ensemble.\n",
    "The key idea behind building the ensemble is that each weak learner contributes its expertise to the ensemble, correcting specific errors made by the existing model. The learning process is guided by the optimization of a specified loss function, and the weights assigned to each learner depend on its ability to minimize the loss.\n",
    "\n",
    "The sequential and additive nature of building the ensemble is what distinguishes Gradient Boosting from other ensemble methods. The ensemble becomes a weighted sum of weak learners, and the iterative correction process leads to a powerful and accurate predictive model capable of capturing complex patterns in the data.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80709e4-f3e0-49f5-aaab-009c6f87f7f6",
   "metadata": {},
   "source": [
    "## Question-7 :What are the steps involved in constructing the mathematical intuition of Gradient Boostingalgorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56749c8c-a092-4d0f-83dd-964c9ab7e51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Constructing the mathematical intuition behind the Gradient Boosting algorithm involves understanding the optimization process, the role of weak learners, and how the algorithm minimizes a specified loss function. Here are the key steps in building the mathematical intuition of Gradient Boosting:\n",
    "\n",
    "Initialization:\n",
    "\n",
    "Start with an initial prediction, often the mean of the target variable for regression problems.\n",
    "The initial prediction serves as the baseline for the ensemble.\n",
    "Compute Residuals:\n",
    "\n",
    "In each iteration, calculate the residuals by subtracting the current ensemble's prediction from the true labels. Residuals represent the errors made by the current ensemble.\n",
    "Train Weak Learner on Residuals:\n",
    "\n",
    "Introduce a weak learner (e.g., a decision tree) to predict the residuals. This weak learner is trained to capture the patterns and dependencies in the data that were not captured by the current ensemble.\n",
    "Update Ensemble:\n",
    "\n",
    "Add the predictions of the weak learner (scaled by a learning rate) to the current ensemble. This update corrects the errors made by the ensemble up to the current iteration.\n",
    "Repeat Iteratively:\n",
    "\n",
    "Repeat the process for a predefined number of iterations or until a stopping criterion is met.\n",
    "In each iteration, a new weak learner is introduced to predict the residuals, and the ensemble is updated accordingly.\n",
    "Optimization of Loss Function:\n",
    "\n",
    "The algorithm minimizes a specified loss function during each iteration. Common loss functions include mean squared error for regression problems and cross-entropy for classification problems.\n",
    "The optimization involves adjusting the parameters of the weak learner to minimize the loss, effectively moving in the direction of the negative gradient of the loss function.\n",
    "Learning Rate:\n",
    "\n",
    "The learning rate controls the contribution of each weak learner to the overall ensemble. A lower learning rate requires more weak learners for the same level of accuracy but can improve generalization.\n",
    "Combination of Predictions:\n",
    "\n",
    "The final prediction of the Gradient Boosting model is the sum of the predictions from all weak learners, each multiplied by its learning rate.\n",
    "The ensemble becomes a weighted sum of weak learners, and the weights are determined by their impact on minimizing the loss function.\n",
    "Regularization (Optional):\n",
    "\n",
    "Gradient Boosting algorithms may include regularization techniques to prevent overfitting. Regularization terms can be added to the loss function to penalize complexity in the weak learners.\n",
    "Sequential Learning:\n",
    "\n",
    "The sequential learning process ensures that each weak learner focuses on the errors made by the existing ensemble. This sequential correction improves the model's accuracy with each iteration.\n",
    "By understanding these steps and the underlying mathematics, one can appreciate how Gradient Boosting constructs a strong ensemble model through the sequential addition of weak learners. The optimization of the loss function guides the learning process, and the combination of weak learners adapts the model to complex patterns in the data.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa0202a-b401-4b76-a9f0-5cc1bb0731a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

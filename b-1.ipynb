{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "060fde30-3bf4-4da8-a23e-27e0f0bd9aed",
   "metadata": {},
   "source": [
    "## Question-1 :What is boosting in machine learning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692bc5d3-86e9-4011-a243-02c30813c898",
   "metadata": {},
   "outputs": [],
   "source": [
    "Boosting is a machine learning ensemble technique designed to improve the performance of weak learners (models that perform slightly better than random chance) by combining them into a strong learner. The general idea is to sequentially train a series of weak models, where each model corrects the errors made by its predecessors.\n",
    "\n",
    "The boosting process involves assigning weights to the training instances, with more emphasis given to the instances that were misclassified by the previous models. This way, the subsequent weak learners focus on the mistakes of the earlier ones, gradually improving the overall performance of the ensemble.\n",
    "\n",
    "Popular boosting algorithms include AdaBoost (Adaptive Boosting), Gradient Boosting, and XGBoost. Each of these algorithms has its variations and strengths, but they share the fundamental principle of boosting by iteratively adjusting the model to address the weaknesses of the ensemble. Boosting often results in highly accurate and robust models, making it a powerful technique in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f56a0e2-3cb5-46fa-8831-64fbbcf47269",
   "metadata": {},
   "source": [
    "## Question-2 :What are the advantages and limitations of using boosting techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639e58d8-29db-4215-9853-fe405ff9f7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Improved Accuracy: Boosting typically leads to higher accuracy compared to individual weak learners. The ensemble nature of boosting helps correct errors and focuses on difficult-to-classify instances.\n",
    "\n",
    "Handles Complex Relationships: Boosting can capture complex relationships in the data by combining multiple weak learners, enabling the model to learn intricate patterns and dependencies.\n",
    "\n",
    "Robustness to Overfitting: Boosting techniques often have mechanisms to control overfitting, such as regularization parameters and early stopping criteria, which helps prevent the model from memorizing the training data.\n",
    "\n",
    "Versatility: Boosting can be applied to various types of weak learners and is not restricted to specific algorithms. It is compatible with decision trees, linear models, and other types of models.\n",
    "\n",
    "Feature Importance: Boosting algorithms provide insights into feature importance, helping to identify which features contribute the most to the predictive performance.\n",
    "\n",
    "Limitations of Boosting Techniques:\n",
    "\n",
    "Sensitivity to Noisy Data: Boosting can be sensitive to noisy data and outliers. Noisy data points that are misclassified by early weak learners might be given too much emphasis in subsequent iterations, potentially leading to overfitting.\n",
    "\n",
    "Computationally Intensive: Training a boosting model can be computationally intensive, especially if the weak learners are complex or the dataset is large. This can make boosting less practical for real-time applications or on resource-constrained devices.\n",
    "\n",
    "Black-Box Nature: Boosting models, especially those based on decision trees, can be considered as black-box models, making it challenging to interpret and understand the reasoning behind specific predictions.\n",
    "\n",
    "Vulnerability to Underlying Weak Models: If the weak learners are too complex or too weak, boosting might not perform well. The success of boosting depends on having a series of weak learners that are sufficiently diverse yet capable of learning from the data.\n",
    "\n",
    "Risk of Overfitting: While boosting is designed to reduce overfitting, there is still a risk, especially if the number of boosting rounds is not appropriately tuned. Overfitting may occur if the model is too complex or if it continues learning noise in the data.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9cebb6-61ee-4877-926b-d16ff1e407ea",
   "metadata": {},
   "source": [
    "## Question-3 :Explain how boosting works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0c6398-8dde-49a2-9d6f-80a0209a9d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "Boosting is an ensemble learning technique that combines the predictions of multiple weak learners to create a strong learner. The general process of boosting involves sequentially training a series of weak models, with each subsequent model giving more emphasis to the instances that were misclassified by its predecessors. Here's a step-by-step explanation of how boosting works:\n",
    "\n",
    "Initialize Weights: Assign equal weights to all training instances. These weights determine the importance of each instance in the training process.\n",
    "\n",
    "Train Weak Learner: Train a weak learner (a model that performs slightly better than random chance) on the training data, considering the weights assigned to each instance. The weak learner's goal is to minimize the errors on the weighted instances.\n",
    "\n",
    "Compute Error: Calculate the error of the weak learner by comparing its predictions to the actual labels. Instances that were misclassified receive higher weights for the next iteration.\n",
    "\n",
    "Update Weights: Increase the weights of misclassified instances, making them more influential in the next iteration. This focuses the attention of the next weak learner on the mistakes made by the previous one.\n",
    "\n",
    "Train Next Weak Learner: Train another weak learner on the updated training data, giving more emphasis to the misclassified instances. This process is repeated for a predetermined number of iterations or until a certain level of accuracy is achieved.\n",
    "\n",
    "Combine Weak Learners: Combine the predictions of all weak learners, usually using a weighted sum. The weights are determined by the performance of each weak learner, with more accurate models getting higher weights.\n",
    "\n",
    "Final Prediction: The combined predictions of the weak learners result in the final prediction of the boosting model. In regression problems, this might be a simple sum of the predictions, while in classification, it could be a weighted voting mechanism.\n",
    "\n",
    "The boosting process continues until a specified number of weak learners are trained or until a certain performance metric is reached. Common boosting algorithms include AdaBoost, Gradient Boosting, and XGBoost, each with its own specific methods for updating weights and combining weak learners. The key idea is to iteratively correct errors and focus on instances that are challenging for the current ensemble, leading to a more accurate and robust model.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549cbab8-b576-496c-8226-0df060b675a4",
   "metadata": {},
   "source": [
    "## Question-4 : What are the different types of boosting algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fef358-9b16-4713-8279-9728ac34b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are several boosting algorithms, each with its own characteristics and variations. Some of the popular boosting algorithms include:\n",
    "\n",
    "AdaBoost (Adaptive Boosting):\n",
    "\n",
    "AdaBoost is one of the earliest and most well-known boosting algorithms.\n",
    "It assigns weights to training instances and adjusts them based on the errors made by weak learners.\n",
    "It sequentially trains a series of weak learners, with each new learner focusing on the instances that were misclassified by the previous ones.\n",
    "Gradient Boosting:\n",
    "\n",
    "Gradient Boosting builds an ensemble of decision trees, typically shallow trees known as weak learners.\n",
    "It minimizes a loss function by adding weak learners in a forward stage-wise manner.\n",
    "The algorithm fits new trees to the residuals (the differences between actual and predicted values) of the previous trees, gradually improving the model's predictions.\n",
    "XGBoost (Extreme Gradient Boosting):\n",
    "\n",
    "XGBoost is an optimized and scalable version of gradient boosting.\n",
    "It includes additional regularization terms to control overfitting and incorporates parallel computing techniques for faster training.\n",
    "XGBoost is known for its efficiency and often outperforms other boosting algorithms.\n",
    "LightGBM (Light Gradient Boosting Machine):\n",
    "\n",
    "LightGBM is another gradient boosting framework designed for efficiency and speed.\n",
    "It uses a histogram-based learning method, which speeds up the training process by binning continuous features.\n",
    "LightGBM is particularly well-suited for large datasets and high-dimensional data.\n",
    "CatBoost:\n",
    "\n",
    "CatBoost is a boosting algorithm developed by Yandex and is designed to handle categorical features effectively.\n",
    "It automatically handles categorical variables without the need for extensive pre-processing.\n",
    "CatBoost incorporates techniques to reduce overfitting and has good out-of-the-box performance.\n",
    "Stochastic Gradient Boosting:\n",
    "\n",
    "Stochastic Gradient Boosting (SGDBoost) is a variation of gradient boosting that introduces stochasticity into the training process.\n",
    "It uses a subset of the training data in each iteration, adding randomness to the model's learning process.\n",
    "This can enhance the model's generalization performance and reduce the risk of overfitting.\n",
    "These boosting algorithms share the common principle of sequentially training weak learners and combining their predictions to form a strong ensemble model. While the core concept remains the same, each algorithm introduces unique strategies and optimizations to improve training speed, accuracy, and generalization across different types of datasets.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045f3a96-bf6e-4024-a618-901c469302a2",
   "metadata": {},
   "source": [
    "## Question-5 :What are some common parameters in boosting algorithms?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bc9cb8-9a54-41ba-af32-0b103a1c1ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "oosting algorithms, such as AdaBoost, Gradient Boosting, XGBoost, and others, have numerous parameters that can be tuned to optimize model performance. Here are some common parameters you might encounter:\n",
    "\n",
    "Number of Trees (n_estimators):\n",
    "\n",
    "Represents the total number of weak learners (trees) in the ensemble.\n",
    "Increasing the number of trees can lead to better performance, but it also increases computational cost.\n",
    "Learning Rate (or shrinkage):\n",
    "\n",
    "Controls the contribution of each weak learner to the overall ensemble.\n",
    "Lower values require more weak learners for the same level of accuracy but can improve generalization.\n",
    "Tree Depth (max_depth):\n",
    "\n",
    "Determines the maximum depth of each weak learner (tree) in the ensemble.\n",
    "Deeper trees can capture more complex patterns but may lead to overfitting.\n",
    "Subsample:\n",
    "\n",
    "Represents the fraction of training instances used to fit each weak learner.\n",
    "Values less than 1.0 introduce stochasticity, potentially improving generalization.\n",
    "Column (Feature) Subsampling:\n",
    "\n",
    "Specifies the fraction of features randomly chosen to grow each tree.\n",
    "Helps reduce overfitting and improves the diversity of weak learners.\n",
    "Regularization Parameters:\n",
    "\n",
    "Depending on the algorithm, there may be parameters like alpha or lambda controlling L1 or L2 regularization to prevent overfitting.\n",
    "Loss Function:\n",
    "\n",
    "Defines the objective function to be minimized during training.\n",
    "Common loss functions include mean squared error for regression and log loss (cross-entropy) for classification.\n",
    "Gamma (min_child_weight):\n",
    "\n",
    "Applicable to tree-based algorithms like XGBoost.\n",
    "Specifies the minimum sum of instance weight (hessian) needed in a child.\n",
    "Higher values lead to more conservative tree growth.\n",
    "Scale Pos Weight:\n",
    "\n",
    "Relevant for imbalanced classification tasks.\n",
    "Balances the positive and negative weights, giving more importance to the minority class.\n",
    "Early Stopping:\n",
    "\n",
    "Allows training to stop when a certain metric on a validation set ceases to improve, preventing overfitting.\n",
    "Categorical Feature Handling:\n",
    "\n",
    "Some algorithms, like CatBoost, have specific parameters for handling categorical features efficiently.\n",
    "It's crucial to carefully tune these parameters based on the characteristics of your dataset and the specific boosting algorithm you're using. Grid search or random search can be employed to find the optimal combination of hyperparameters. Additionally, cross-validation helps assess the model's generalization performance across different parameter settings.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa4c286-5fac-4e11-a193-4dd0db978846",
   "metadata": {},
   "source": [
    "## Question-6 :How do boosting algorithms combine weak learners to create a strong learner?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bffff0-79ee-48c1-9b17-50933a6d6273",
   "metadata": {},
   "outputs": [],
   "source": [
    "Boosting algorithms combine weak learners to create a strong learner through a process that involves assigning weights to individual weak learners and their predictions. The combination is typically done through a weighted sum in the case of regression or a weighted voting mechanism for classification. Here's a more detailed explanation of how boosting algorithms combine weak learners:\n",
    "\n",
    "Weighted Voting (Classification):\n",
    "\n",
    "In classification problems, each weak learner makes predictions for each instance, assigning class labels. These predictions are combined through a weighted voting mechanism.\n",
    "The weights assigned to each weak learner are determined based on its performance during training. More accurate models are given higher weights, indicating greater influence on the final prediction.\n",
    "The final prediction is often determined by a majority vote, where the class with the highest weighted sum of votes is selected.\n",
    "Weighted Sum (Regression):\n",
    "\n",
    "In regression problems, weak learners produce continuous predictions. Each weak learner's prediction is multiplied by its weight, and the weighted predictions are summed.\n",
    "Similar to the classification case, the weights reflect the performance of each weak learner. Models with better performance are assigned higher weights.\n",
    "The final prediction is the sum of the weighted predictions.\n",
    "Sequential Learning:\n",
    "\n",
    "Boosting algorithms employ a sequential learning process where each weak learner is trained to correct the errors made by its predecessors.\n",
    "Weights are assigned to training instances, and these weights are adjusted based on the errors made by the previous weak learners. Instances that are misclassified receive higher weights, leading the next weak learner to focus on those instances.\n",
    "The process continues iteratively, with each new weak learner improving the ensemble's performance by addressing the mistakes of the previous models.\n",
    "Gradient Descent Optimization (Gradient Boosting):\n",
    "\n",
    "In gradient boosting, the weak learners are typically decision trees, and the optimization process involves minimizing a loss function.\n",
    "Each weak learner is fitted to the residuals (the differences between actual and predicted values) of the previous weak learners.\n",
    "The weights of the weak learners are determined by the contribution they make to minimizing the overall loss function.\n",
    "Regularization:\n",
    "\n",
    "Boosting algorithms often incorporate regularization techniques to prevent overfitting and control the complexity of the ensemble.\n",
    "Regularization parameters may penalize the weights assigned to the weak learners, helping to avoid excessive reliance on a single model.\n",
    "The combination of weak learners in boosting is a crucial aspect of the algorithm, as it leverages the strengths of multiple models to create a robust and accurate ensemble. The iterative process of assigning weights and correcting errors allows boosting to adapt and improve over time, resulting in a strong learner capable of capturing complex patterns in the data.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93d6fb2-8b04-4a37-8754-7fa8428a4e36",
   "metadata": {},
   "source": [
    "## Question-7 :Explain the concept of AdaBoost algorithm and its working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8eef21-2eb0-424d-bbbc-ca7b39643100",
   "metadata": {},
   "outputs": [],
   "source": [
    "AdaBoost, short for Adaptive Boosting, is an ensemble learning algorithm designed to improve the performance of weak learners and create a strong learner. The core idea behind AdaBoost is to sequentially train a series of weak models (often decision trees), assigning different weights to training instances at each iteration. The weights are adjusted to focus on instances that are misclassified by the previous weak learners.\n",
    "\n",
    "Here is a step-by-step explanation of how the AdaBoost algorithm works:\n",
    "\n",
    "Initialize Weights:\n",
    "\n",
    "Assign equal weights to all training instances. Initially, each instance has an equal influence on the training process.\n",
    "Train Weak Learner:\n",
    "\n",
    "Train a weak learner (e.g., a decision tree with limited depth) on the training data, considering the weights assigned to each instance.\n",
    "The weak learner aims to minimize the weighted error, where the weights emphasize the importance of misclassified instances.\n",
    "Compute Error:\n",
    "\n",
    "Calculate the error of the weak learner by comparing its predictions to the actual labels. The error is a weighted sum, with misclassified instances receiving higher weights.\n",
    "Compute Learner Weight:\n",
    "\n",
    "Calculate the weight of the weak learner based on its error. More accurate weak learners receive higher weights, indicating greater influence on the final ensemble.\n",
    "Update Instance Weights:\n",
    "\n",
    "Update the weights of training instances. Instances that were misclassified by the weak learner receive higher weights, making them more influential in the next iteration.\n",
    "The idea is to give more attention to the instances that are challenging for the current ensemble.\n",
    "Repeat Steps 2-5:\n",
    "\n",
    "Repeat the process for a predetermined number of iterations or until a certain level of accuracy is achieved.\n",
    "Each new weak learner focuses on the errors made by its predecessors, gradually improving the overall performance of the ensemble.\n",
    "Combine Weak Learners:\n",
    "\n",
    "Combine the predictions of all weak learners into a final prediction. The combination is often done through a weighted voting mechanism, where the weights are determined by the accuracy of each weak learner.\n",
    "Final Prediction:\n",
    "\n",
    "The combined predictions of the weak learners result in the final prediction of the AdaBoost model. In classification, this might be a weighted majority vote, while in regression, it could be a weighted sum of predictions.\n",
    "AdaBoost is effective because it adapts to the difficulty of instances in the training data, giving more emphasis to those that are challenging to classify. The final ensemble benefits from the diversity of weak learners and their ability to specialize in different regions of the feature space. AdaBoost is known for its simplicity and effectiveness, and it is a foundational algorithm in the field of ensemble learning.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503c8393-ba49-447d-989e-027b01ae75fa",
   "metadata": {},
   "source": [
    "## Question-8 :What is the loss function used in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641173f4-9cc2-49f7-97d5-bc1d7d0caa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "AdaBoost does not use a traditional loss function like other boosting algorithms such as gradient boosting. Instead, it focuses on minimizing the weighted classification error. The goal of AdaBoost is to iteratively train weak learners (typically decision stumps or shallow trees) and assign weights to training instances in such a way that the subsequent weak learners give more attention to the instances that were misclassified by the previous ones.\n",
    "\n",
    "The weighted classification error at each iteration is used to assess the performance of the weak learner. The error is calculated as the sum of the weights of the misclassified instances divided by the total sum of weights. This error is then used to determine the weight of the weak learner in the final ensemble.\n",
    "\n",
    "The mathematical representation of the weighted classification error (err_m) at a given iteration (m) is as follows:\n",
    "=\n",
    "∑\n",
    "=\n",
    "1\n",
    "⋅\n",
    "1\n",
    "(\n",
    "ℎ\n",
    "(\n",
    ")\n",
    "≠\n",
    ")\n",
    "∑\n",
    "=\n",
    "1\n",
    "err \n",
    "m\n",
    "​\n",
    " = \n",
    "∑ \n",
    "i=1\n",
    "N\n",
    "​\n",
    " w \n",
    "i\n",
    "​\n",
    " \n",
    "∑ \n",
    "i=1\n",
    "N\n",
    "​\n",
    " w \n",
    "i\n",
    "​\n",
    " ⋅1(h \n",
    "m\n",
    "​\n",
    " (x \n",
    "i\n",
    "​\n",
    " )\n",
    "=y \n",
    "i\n",
    "​\n",
    " )\n",
    "​\n",
    " \n",
    "\n",
    "Where:\n",
    "N is the number of training instances.\n",
    "w \n",
    "i\n",
    "​\n",
    "  is the weight assigned to the ith training instance.\n",
    "ℎ\n",
    "(\n",
    ")\n",
    "h \n",
    "m\n",
    "​\n",
    " (x \n",
    "i\n",
    "​\n",
    " ) is the prediction of the weak learner at iteration m for the ith instance.\n",
    "y \n",
    "i\n",
    "​\n",
    "  is the true label of the ith instance.\n",
    "1\n",
    "(\n",
    "⋅\n",
    ")\n",
    "1(⋅) is the indicator function, which equals 1 if the condition inside is true and 0 otherwise.\n",
    "The weight of the weak learner at iteration m (alpha_m) is then calculated as:\n",
    "=\n",
    "1\n",
    "2\n",
    "⋅\n",
    "ln\n",
    "⁡\n",
    "(\n",
    "1\n",
    "−\n",
    ")\n",
    "α \n",
    "m\n",
    "​\n",
    " = \n",
    "2\n",
    "1\n",
    "​\n",
    " ⋅ln( \n",
    "err \n",
    "m\n",
    "​\n",
    " \n",
    "1−err \n",
    "m\n",
    "​\n",
    " \n",
    "​\n",
    " )\n",
    "\n",
    "The alpha_m values are used to weigh the contribution of each weak learner in the final ensemble. More accurate weak learners (lower error) receive higher weights, indicating greater influence on the ensemble's final prediction.\n",
    "\n",
    "In summary, AdaBoost minimizes the weighted classification error to guide the training process and assign appropriate weights to weak learners based on their performance. This weighting mechanism ensures that subsequent weak learners focus on instances that were challenging for the previous ones, leading to an ensemble that adapts to the difficulty of the training data.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d3a1ae-a7af-4951-b927-b3194eac1ddc",
   "metadata": {},
   "source": [
    "## Question-9 :How does the AdaBoost algorithm update the weights of misclassified samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9959bf-13ae-4e93-9252-66f028a319c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "The AdaBoost algorithm updates the weights of misclassified samples in order to give more emphasis to those instances that were difficult for the current ensemble of weak learners to classify correctly. The goal is to guide the subsequent weak learners to focus on the mistakes made by their predecessors. The weight update process can be summarized as follows:\n",
    "\n",
    "Initialize Weights:\n",
    "\n",
    "Assign equal weights to all training instances at the beginning.\n",
    "Train Weak Learner:\n",
    "\n",
    "Train a weak learner on the current set of training instances, considering the weights assigned to each instance.\n",
    "Compute Error:\n",
    "\n",
    "Calculate the error of the weak learner by comparing its predictions to the actual labels. The error is computed as the sum of the weights of the misclassified instances divided by the total sum of weights.\n",
    "Compute Learner Weight:\n",
    "\n",
    "Calculate the weight of the weak learner based on its error. The learner weight (\n",
    "α) is determined by the performance of the weak learner and is computed using the formula:\n",
    "=\n",
    "1\n",
    "2\n",
    "⋅\n",
    "ln\n",
    "⁡\n",
    "(\n",
    "1\n",
    "−\n",
    "error\n",
    "error\n",
    ")\n",
    "α= \n",
    "2\n",
    "1\n",
    "​\n",
    " ⋅ln( \n",
    "error\n",
    "1−error\n",
    "​\n",
    " )\n",
    "Update Weights:\n",
    "\n",
    "Update the weights of the training instances based on whether they were correctly or incorrectly classified by the weak learner.\n",
    "For correctly classified instances (\n",
    "ℎ\n",
    "(\n",
    ")\n",
    "=\n",
    "h \n",
    "m\n",
    "​\n",
    " (x \n",
    "i\n",
    "​\n",
    " )=y \n",
    "i\n",
    "​\n",
    " ):\n",
    "(\n",
    "+\n",
    "1\n",
    ")\n",
    "=\n",
    "(\n",
    ")\n",
    "⋅\n",
    "exp\n",
    "⁡\n",
    "(\n",
    "−\n",
    ")\n",
    "w \n",
    "i\n",
    "(m+1)\n",
    "​\n",
    " =w \n",
    "i\n",
    "(m)\n",
    "​\n",
    " ⋅exp(−α)\n",
    "For misclassified instances (\n",
    "ℎ\n",
    "(\n",
    ")\n",
    "≠\n",
    "h \n",
    "m\n",
    "​\n",
    " (x \n",
    "i\n",
    "​\n",
    " )\n",
    "\n",
    "=y \n",
    "i\n",
    "​\n",
    " ):\n",
    "(\n",
    "+\n",
    "1\n",
    ")\n",
    "=\n",
    "(\n",
    ")\n",
    "⋅\n",
    "exp\n",
    "⁡\n",
    "(\n",
    ")\n",
    "w \n",
    "i\n",
    "(m+1)\n",
    "​\n",
    " =w \n",
    "i\n",
    "(m)\n",
    "​\n",
    " ⋅exp(α)\n",
    "Here, \n",
    "(\n",
    "+\n",
    "1\n",
    ")\n",
    "w \n",
    "i\n",
    "(m+1)\n",
    "​\n",
    "  represents the updated weight for the ith instance after the mth iteration.\n",
    "Normalize Weights:\n",
    "\n",
    "After updating the weights, normalize them to ensure that they sum to 1. This normalization step helps maintain the interpretation of weights as probabilities.\n",
    "Repeat the Process:\n",
    "\n",
    "Repeat the entire process for a predetermined number of iterations or until a certain stopping criterion is met.\n",
    "Each iteration focuses more on the instances that were misclassified in the previous iterations.\n",
    "The weight update mechanism ensures that misclassified instances receive higher weights, making them more influential in subsequent training iterations. This iterative process of assigning weights and training weak learners continues until the desired number of weak learners is reached. The final ensemble combines the predictions of all weak learners with their respective weights to make a strong prediction on new, unseen data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df222b1-0647-4ea8-ae28-f353c107db0a",
   "metadata": {},
   "source": [
    "## Question-10 :What is the effect of increasing the number of estimators in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a36d623-bfc7-4909-a6b2-0e1b67989da5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
